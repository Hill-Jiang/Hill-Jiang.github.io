{"posts":[{"title":"不合理的设计实现是如何引发性能问题的","content":"背景 前段时间出了不少性能相关的市场反馈，某大型连锁店客户在使用云平台时频繁出现卡顿现象，基本不可用；甚至在其使用期间，由于大量占用了系统资源服务，导致后端接口响应时间明显边长，影响了其他用户的支持使用 客户配置信息 连锁店项目，所有设备放在同一个项目中进行管理 每个一级区域对应一个城市，每个二级区域对应一个门店 每个门店添加一个NVR，关联若干IPC通道 设备 数量 总监控点数 10000 NVR 2000 每个 NVR 下的通道数 1 到 16 区域 数量 一级区域 500 二级区域 每个一级区域下包含 1-50 个二级区域，共 2000 个二级区域 问题原因 假设有以下带有层级的树状区域结构： |-一级区域1 |-二级区域1-1 |-二级区域1-2 |-二级区域1-3 ... |-一级区域2 |-二级区域2-1 |-二级区域2-2 |-二级区域2-3 ... ... 有多个一级区域，且每个区域下有多个二级区域 进入设备管理页面，前端会发送如下请求给后端： 请求 请求参数 作用 getRootRegions - 获取所有一级区域 getRegionChildren rootRegionId 获取特定一级区域下的所有二级区域 getDeviceList regionId 获取特定区域下的设备列表 区域相关的所有请求，不分页，也就是 getRootRegions 会响应所有一级区域的信息，getRegionChildren 会响应所有特定区域下的子区域，不管响应结果会有多少个 进入页面时，会完整请求所有区域，以及所有区域下的设备信息 区域提供搜索功能，搜索由前端实现，所以要完整加载所有区域信息后才能实现搜索 请求数量 = 1（getRootRegions） + 总一级区域数量（getRegionChildren） + 总区域数量（getDeviceList） 这样一来，一进入页面，就会发送多个请求，请求数量随区域数量的增加而线性增加，导致在区域数量很多时，并行发送了大量请求给后端 从该客户的配置情况来看，前端会发送1+500+(500+2000)=3001个请求，并行发大量请求带来的影响有： 浏览器一般会限制并行请求数量，前面的请求未得到响应时，后发的请求暂时阻塞，导致大量请求被阻塞，响应时间长，响应可能出现超时导致服务不可用 并行请求数量太多，服务器资源占用突增，后端服务器响应时间变长 优化方案 该问题虽然可以认为是性能问题（配置少的情况下不会出现、配置多的情况下出现），但是从直接原因来看，是前后端设计实现不合理导致的，因此针对此页面提出如下几个改进方案： 所有获取区域功能，添加分页接口，一次获取有限数量的信息，直到页面滚动或者用户手动点击加载更多才按需加载更多区域 所有需要搜索区域的功能，添加搜索接口，搜索由后端实现 所有请求设备的接口，只请求当前选中区域的设备、未选中区域的设备不请求 如此一来，就能大大减少页面发送的请求数量，而对用户体验基本又不会有影响，后续采用此方案优化效果明显 引入的其他问题 实际改动提测后，发现涉及到区域勾选+搜索的页面（设置角色权限时，可以批量勾选其拥有权限的区域），会遇到问题。 因为当前是分页加载的，前端在加载完成之前并不知道有多少区域 如果勾选了父区域，子区域也被选中，前端传参直接是父区域ID 考虑如下场景：此时如果取消勾选某个子区域，父区域会变成半选状态，但是如果子区域过多、一页没有加载完，直接确认，前端传参就会变成这一页的所有勾选的子区域ID，剩下没加载的子区域因为前端不知道具体信息，所以不会传给后端，导致实际结果与预期不符 进一步的，涉及勾选后再搜索、搜索后再清除某些已搜索的内容，都会因为前端没有获取过完整的树结构，导致一些不符合预期的结果（讨论过搜索清空已选的方案，认为与实际用户习惯不符，不能接受） 这个问题相对比较棘手，讨论了很久都没有好的解决方案，于是去看了看竞品有没有类似的页面（树组织结构+父子层级+多选） 解决方案 海康云眸社区：https://www.hik-cloud.com/neptune/index.html 有相似功能的有两个页面，其实现还不一样： 父子组织树结构+搜索功能 分页获取 搜索后端完成 父子组织树结构+勾选功能+搜索功能 不分页直接获取完整组织树 搜索前端完成 参考这个实现，又想到是不是可以不分页、前端获取完整组织树、搜索前端完成 之前出现性能问题的场景是，有多个根区域、每个根区域又有子区域，这时候获取完根区域之后再去遍历获取每个根区域的子区域，导致并行发送的请求数量很多 可以考虑新增一个获取完整区域树结构的接口： 输入 projectId，输出所有区域信息 输出区域信息不包含层级关系，通过 regionBranch（regionId 完整路径） 来标示父子接哦故 前端用区域信息来构造完整的区域树（一个请求搞定） 搜索功能仍然有前端来完成，保留勾选状态 父子组织树结构+勾选功能+搜索功能的页面使用频次很少，低频次的完整组织树获取也不会对服务有太大影响 从竞品情况来看，3000+个区域，单个接口的响应大小在200K左右（其regionId为40位string，相对较长），单个接口的响应时间在100ms左右，都可以接受 后端接口性能上，响应的都是数据库已有字段，projectId 已加索引，性能风险不大 SELECT regionId, regionName, regionBranch, order form region_info where projectId='xxx'; 需要再评估一下前端性能 POST /tums/resources/getRegionTree Request: { &quot;projectId&quot;: &quot;123&quot; } Response: { &quot;result&quot;: [ { &quot;regionId&quot;：&quot;1&quot;, &quot;regionName&quot;: &quot;一&quot;, &quot;regionBranch&quot;: &quot;1-&quot;， &quot;order&quot;: 1 }, { &quot;regionId&quot;: &quot;2&quot;, &quot;regionName&quot;: &quot;二&quot;, &quot;regionBranch&quot;: &quot;1-2-&quot;, &quot;order&quot;: 1 } ], &quot;error_code&quot;: 0 } 总结与反思 遇到性能问题，先从设计实现的角度去看看，有没有实现不合理的地方，再去着手考虑优化 前端资源按需加载，一次加载不必要的资源，不仅加大了服务压力，还导致响应时间延长影响用户体验 一个问题有解决方案后，不一定对所有业务功能都通用，要灵活考虑 ","link":"https://hill-jiang.github.io/post/bu-he-li-de-she-ji-shi-xian-shi-ru-he-yin-fa-xing-neng-wen-ti-de/"},{"title":"JMeter实现并行发送携带列表参数中某一项的请求","content":"需求背景 最近项目新增了一个后端接口，功能大概就是传入区域的ID，响应该区域下所有的设备信息。 针对这个新增接口做性能测试，需要测试响应成功率、最大吞吐量等指标，涉及到HTTP请求的并行发送和结果信息收集。这次打算不用之前自己写python脚本的方式，改用JMeter试试看。 用JMeter实现上述测试需求的过程中，遇到了一些问题，自己摸索了一下解决方案，记录一下。 问题 使用JMeter自带的JSON提取器，从获取区域信息的响应中获取regionId的值后，被保存到了一个列表中，实际上是一组JMeterVariables。 我需要遍历这个列表，再用这个列表中的每一项作为请求体来并行发送请求。比如获取到了50个regionId，并行发送50个请求，每个请求的请求体包含一个regionId的值，类似： for i in range(50): payload = {&quot;regionId&quot;: region_id[i]} 一开始打算用JMeter的并行处理插件 parallel controller 和 forEach controller 来实现循环遍历和并行发送请求，但是发现无论是 parallel 在 forEach 控制器下，还是 forEach 在 parallel 下，都还是串行发送的请求。 猜测这个问题可能和插件实现机制有关，每个 forEach controller 都还是在一个线程中串行创建的。 不用 forEach，用 while 控制器 + 计数器来实现遍历，也是一样的结果。 解决方案 既然在同一个线程组里面没法用 parallel controller 来实现并行，那只能考虑用多个线程组来实现了，这就又涉及到了线程组之间的变量传递，我需要把上一个线程组中获取到的变量列表传递到下一个线程组中去。 折腾了一圈，最终用 BeanShell取样器+CSV文件存储的方式实现了。 添加一个 BeanShell取样器。在取样器中，编写脚本，把 regionId 的列表长度设置为全局变量，再把每个 regionId 都写入 csv 文件中去。这里不循环遍历把所有 regionId 都写到全局变量是因为研究了半天没弄出来__setProperty方法怎么加上变量索引。 FileWriter fstream = new FileWriter(&quot;F:/Documents/JMeter/result/src/regionId.csv&quot;,false); BufferedWriter out = new BufferedWriter(fstream); ${__setProperty(regionNum,${regionIds_matchNr},)} for(int i=1;i&lt;=${regionIds_matchNr};i++) { String regionId_value = vars.get(&quot;regionIds_&quot; + i); // log.info(regionId_value); out.write(regionId_value+&quot;\\n&quot;); } out.close(); fstream.close(); 创建一个线程组，线程数设置为${__property(regionNum)}，表示并行运行 regionId 数组长度个线程。Ramp-Up 时间可以根据测试需要自行设置合适值。循环次数设置为 1 ，并不需要重复运行。 在新创建的线程组中，添加 CSV数据文件设置，读取在上一个线程组中写入的 csv 文件，保存到变量中，比如 regionId，每个线程会自动把 csv 文件中当前线程数对应的行写入这个变量。 创建 HTTP 请求，请求参数设置为{&quot;regionId&quot;: &quot;${regionId}&quot;}即可。 不要忘了在 Test Plan 中勾选“独立运行每个线程组”，否则所有线程组将一起启动。 可以看到，请求基本上都是并行发送的了。 ","link":"https://hill-jiang.github.io/post/jmeter-shi-xian-bing-xing-fa-song-xi-dai-lie-biao-can-shu-zhong-mou-yi-xiang-de-qing-qiu/"},{"title":"git升级踩坑","content":"git升级踩坑 问题 最近升级了一下 pycharm，重启提示 git 版本不受支持，然后就找了个最新版本的 git 升级 升级完之后，发现 git pull 报错，Permission denied (publickey). 排查 这个报错之前见过，没配 SSH key 就是这个报错 一开始以为是 git 升级完要重新配一下 SSH key，心想怎么这么不智能 重新生成完了，在 gerrit 上也重新 Add 新生成的 SSH key 了，报错还是一样，重新 git clone 也是一样的报错 眉头一皱，一搜，有大坑。 问题原因 GIT 2.33.1 版本集成了最新的 OpenSSH v8.8p1版本，此版本放弃了历史相当悠久的 rsa-sha1的支持。 2.33 以上的版本，如果再用 rsa 来生成 SSH Key，就会产生问题，需要更换密钥 解决方案 方案一 如果你急需访问仓库，而暂时不想修改密钥，可以在密钥所在的. ssh 目录下的 config 文件（没有的话自行创建）添加如下配置即可访问。 Host git.xxxxxx.com HostkeyAlgorithms +ssh-rsa PubkeyAcceptedAlgorithms +ssh-rsa 方案二 重新生成更安全的密钥。 在生成之前，要确定服务器是否支持相应的密钥加密算法。 使用 ECDSA 或者 ED25519 算法替代 RSA 以一个不错的选择。 ssh-keygen -t ed25519 -C &quot;your@example.email&quot; 方案三 回滚 git 版本 方案四 windows 版本的 git 安装的时候，可以选择“use external OpenSSH”。这样可以不使用内置的 OpenSSH。可以指定一个可用的 OpenSSH 安装路径。 最终确认 gerrit 支持更安全的密钥算法，就用方案二解决了。 ","link":"https://hill-jiang.github.io/post/git-sheng-ji-cai-keng/"},{"title":"线程池参数设置不合理导致的消息丢失问题","content":"问题背景 充电桩使用4G流量卡上网，在4G信号不稳定的情况下，可能出现设备离线 生产环境中，偶现充电桩掉线后，始终没有在云平台重新上线，导致用户扫码提示设备故障无法使用 而实际设备已经在线，在云平台 WEB 上手动同步设备信息后，设备恢复在线状态，可以正常使用 业务流程 以设备上线为例，设备首先和云平台的设备连接层建立socket连接交互，连接层模块会产生一个上线事件，发到kafka的topic上，下游模块消费后，经过业务处理，转发给云平台的设备管理模块，设备管理模块收到消息后修改对应的设备在线状态、触发消息提醒。 participant 设备 as d participant 设备连接层 as c participant kafka as k participant 下游模块 as h participant 云平台 as b d-&gt;c: 建立连接 note over d, c: 长连接 c-&gt;k: 设备上线事件 k-&gt;h: 消费消息 note over h: 业务处理 h-&gt;b: 消息转发 note over b: 产生消息提示 note over b: 修改设备在线状态 根据日志，初步定位问题原因是云平台未收到设备上线的消息，导致无法正确修改设备在线状态。 问题定位 测试环境使用约 50 个虚拟设备反复触发离线、上线逻辑，挂载数小时可复现。 继续添加日志定位问题： 在kafka处添加Debug打印，确认设备消息是否成功投递到kafka：消息丢失时间点，消息已成功投递到kafka 新增一个消费者来消费kafka的消息：消息丢失时，新增的消费者正常消费了消息 这两部分说明kafka消息队列没有问题，下游模块消费消息或是业务处理出现了问题。 下游模块处理流程： kafka 消费线程对包进行解析、封装 打印日志 限流判断，如果 TPS 大于 400 则进行限流处理 扔给业务线程池处理 打印日志 后续其他具体业务流程 2 打印正常、5 没有打印；在测试环境下远没有触发限流，3跳过；因此定位问题出在步骤4，业务线程池的处理出现了问题导致消息丢失。 线程池 基本概念 初始化时就建立好线程资源，避免反复创建新的线程造成的资源开销。 优点 降低资源消耗：减少新建和销毁线程所调用的资源 提高响应速度：任务到达时，不需要等待新建线程后执行任务 提高线程的可管理性：线程是有限的资源，如果创建太多可能会导致系统故障，使用线程池可以做到统一的分配，调用和监控 相关参数 threadPool: corePoolSize: 20 maximumPoolSize : 2000 workQueue : 1000 keepAliveSeconds: 300 第1个参数：corePoolSize 表示常驻核心线程数 如果等于0，则任务执行完成后，没有任何请求进入时销毁线程池的线程 如果大于0，即使本地任务执行完毕，核心线程也不会被销毁 这个值的设置非常关键，设置过大会浪费资源，设置的过小会导致线程频繁地创建或销毁 第2个参数：maximumPoolSize 表示线程池能够容纳同时执行的最大线程数 必须大于或等于1 如果 maximumPoolSize 与 corePoolSize 相等，即是固定大小线程池。 第3个参数：workQueue 表示缓存队列 如果线程池里的线程数大于 corePoolSize，就会放到缓存队列 缓存队列满了会创建新线程到 maximumPoolsize 当请求的线程数大于 maximumPoolSize 时，会执行设定的策略，默认是拒绝创建策略 注意：当线程池里的线程数大于 corePoolSize 且小于 maximumPoolSize 时，这时候再有请求的线程就会放到缓存队列，注意只是放到缓存队列但是不创建新的线程，直到请求的线程存满缓存队列时，才会开始创建新的线程，直到 maxmunPoolSize 就会拒绝创建或者执行提前设定的策略 第4个参数：keepAliveSeconds 表示线程池中的线程空闲时间 当空闲时间达到 keepAliveSeconds 值时，线程被销毁，直到剩下 corePoolSize 个线程为止，避免浪费内存和句柄资源 线程池工作流程大致如下： graph LR A(开始) --&gt; B[提交任务] B --&gt; C{线程池状态是否Running?} C --&gt; |是|D{线程数小于核心数?} D --&gt; |是|E[添加工作线程并执行] E --&gt; F(结束) C --&gt; |否|G[任务拒绝] G --&gt; F D --&gt; |否|H{阻塞队列已满?} H --&gt; |是|I{线程数小于最大线程数?} I --&gt; |是|E I --&gt; |否|G H --&gt; |否|J[添加到阻塞队列, 等待工作线程获取执行] J --&gt; F 拒绝策略 当线程数大于 maximumPoolSize 时，会执行拒绝策略，常见的拒绝策略有四种： CallerRunsPolicy（调用者运行策略） 当触发拒绝策略时，只要线程池没有关闭，就由提交任务的当前线程处理 使用场景：一般在不允许失败的、对性能要求不高、并发量较小的场景下使用，因为线程池一般情况下不会关闭，也就是提交的任务一定会被运行，但是由于是调用者线程自己执行的，当多次提交任务时，就会阻塞后续任务执行，性能和效率自然就慢了。 AbortPolicy（中止策略） 当触发拒绝策略时，直接抛出拒绝执行的异常，中止策略的意思也就是打断当前执行流程 使用场景：这个就没有特殊的场景了，但是有一点要正确处理抛出的异常。ThreadPoolExecutor 中默认的策略就是 AbortPolicy，ExecutorService 接口的系列 ThreadPoolExecutor 因为都没有显示的设置拒绝策略，所以默认的都是这个。但是请注意，ExecutorService 中的线程池实例队列都是无界的，也就是说把内存撑爆了都不会触发拒绝策略。当自己自定义线程池实例时，使用这个策略一定要处理好触发策略时抛的异常，因为他会打断当前的执行流程 DiscardPolicy（丢弃策略） 直接静悄悄的丢弃这个任务，不触发任何动作 使用场景：如果你提交的任务无关紧要，你就可以使用它。因为它就是个空实现，会悄无声息的吞噬你的的任务。所以这个策略基本上不用了 DiscardOldestPolicy（弃老策略） 如果线程池未关闭，就弹出队列头部的元素，然后尝试执行 这个策略还是会丢弃任务，丢弃时也是毫无声息，但是特点是丢弃的是老的未执行的任务，而且是待执行优先级较高的任务。基于这个特性，想到的场景就是，发布消息和修改消息，当消息发布出去后，还未执行，此时更新的消息又来了，这个时候未执行的消息的版本比现在提交的消息版本要低就可以被丢弃了。因为队列中还有可能存在消息版本更低的消息会排队执行，所以在真正处理消息的时候一定要做好消息的版本比较 实际配置的策略为 DiscardOldestPolicy，丢弃队列头部元素，即未执行的最旧的任务。 所以当阻塞队列已满、并且线程数也已经达到了最大线程数的时候，就会执行拒绝策略，导致消息丢失。 最终该问题定位的原因是线上消息压力过大，而配置的线程池参数过小、拒绝策略不合理，导致出现任务拒绝丢失消息。 合理设置线程池参数 首先确定以下几个相关参数： avgTasks，程序每秒需要处理的平均任务数量 maxTasks，程序每秒需要处理的最大任务数量 taskHandleTime，单线程处理一个任务所需要的时间 responsetime，系统允许任务最大的响应时间 peakTime，任务峰值持续时间 根据这几个参数，可以算出核心线程数、任务队列长度、最大线程数、线程空闲时间的推荐值 corePoolSize：常驻核心线程数 核心线程数需要能够满足平均负载 corePoolSize = avgTasks * taskHandleTime maximumPoolSize：最大线程数 在峰值时，最大线程数需要能够处理所有的任务 maximumPoolSize = maxTasks * taskHandleTime workQueue：缓存队列长度 缓存队列的长度是在核心线程满载和最大线程数之间的差距所能处理的任务数量，需要确保在响应时间内，队列中的任务可以被处理 workQueue = (maximumPoolSize - corePoolSize) * responseTime / taskHandleTime keepAliveSeconds：最长线程空闲时间 当负载降低时，可减少线程数量，如果一个线程空闲时间达到 keepAliveTiime，该线程就销毁 keepAliveTiime 是一个经验值，具体的值可能需要根据实际情况进行调整，也可根据任务峰值持续时间 peakTime 来设定 ","link":"https://hill-jiang.github.io/post/xian-cheng-chi-can-shu-she-zhi-bu-he-li-dao-zhi-de-xiao-xi-diu-shi-wen-ti/"},{"title":"报文传输中的 url 编码问题","content":"url 编码 当 url 路径，或者查询参数中带有中文、特殊字符的时候，就需要对 url 进行编码（采用十六进制编码格式）。url 编码的原则是使用安全字符（即没有特殊用途或者特殊意义的字符）去表示那些不安全的字符。 对于特殊字符来说，url 编码就是一个字符 ascii 码的十六进制。（ASCII 码使用 8 位二进制数的组合来表示不同的字符，也就是一个字节的大小。其中常规的 ASCII 码，最高位是 0，所以总共可以表示 128 个字符。） 不过稍微有些变动，需要在前面加上 %。 比如 \\，它的 ascii 码是 92，92 的十六进制是 5c，所以 \\ 的 url 编码就是%5c。 对于中文来说，url 编码是将其转换为 Unicode 码，再转化为十六进制的 UTF-8 编码，最后加上%。 url 编码使用后跟十六进制数字的 &quot;%&quot; 替代不安全的 ASCII 字符。 url 不能包含空格。url 编码通常使用加号（+）或 %20 替代空格。 RFC3986 文档规定，url 中只允许包含以下四种： 英文字母（a-zA-Z） 数字（0-9） -_.~ 4 个特殊字符 所有保留字符，RFC3986 中指定了以下字符为保留字符（英文字符）： ! * ' ( ) ; : @ &amp; = + $, / ? # [ ] 所谓保留字符，就是在 url 中具有特定意义的字符。 常见 url 编码转换表 编码前 编码后 编码前 编码后 编码前 编码后 space %20 * %2A &gt; %3E ! %21 + %2B ? %3F &quot; %22 , %2C @ %40 # %23 - %2D [ %5B $ %24 . %2E \\ %5C % %25 / %2F ] %5D &amp; %26 : %3A ^ %5E ' %27 ; %3B _ %5F ( %28 &lt; %3C ` %60 ) %29 = %3D { %7B | %7C } %7D ~ %7E 测试需要注意的点 集中存储、监控点上墙等场景，传输的是 url 地址，需要重点关注用户名/密码包含所有保留字符的表现，因为后端在处理 url、从中提取用户名、密码、IP 等信息时，可能会遇到保留字符。 设置设备名称等场景，原则上用户输入可以是任意字符，在HTTP传输过程中，为了确保数据的正确性和可靠性，通常需要对payload进行URL编码处理，这也就导致了所有需要进行HTTP传输的场景，客户端需要将原始用户输入进行URL编码传输、而将从服务端收到的数据进行URL解码显示，其中一个环节出现问题就可能导致传输或显示异常。 此时需要关注所有 url 编解码相关的字符。 可能存在风险的字符： 空格 所有保留字符 % 本身 编码后的字符，比如 %20、%21 等 各编程语言的url编解码方式 Python： urllib.parse.quote和urllib.parse.unquote：这是Python标准库中提供的url编解码函数，可以将字符进行url编码或解码。 lang=Python import urllib.parse # url编码 url = 'https://www.example.com/search?q=Python 编程' encoded_url = urllib.parse.quote(url) print(encoded_url) # https%3A//www.example.com/search%3Fq%3DPython%20%E7%BC%96%E7%A8%8B # url解码 decoded_url = urllib.parse.unquote(encoded_url) print(decoded_url) # https://www.example.com/search?q=Python 编程 requests.utils.quote和requests.utils.unquote：这是requests库中提供的url编解码函数，与urllib.parse类似。 lang=Python import requests.utils # url编码 url = 'https://www.example.com/search?q=Python 编程' encoded_url = requests.utils.quote(url) print(encoded_url) # https%3A//www.example.com/search%3Fq%3DPython%20%E7%BC%96%E7%A8%8B # url解码 decoded_url = requests.utils.unquote(encoded_url) print(decoded_url) # https://www.example.com/search?q=Python 编程 Java： java.net.URLEncoder和java.net.URLDecoder：这是Java标准库中提供的URL编解码类，可以将字符串进行URL编码或解码 lang=Java import java.net.URLEncoder; import java.net.URLDecoder; // URL编码 String url = &quot;https://www.example.com/search?q=Java 编程&quot;; String encodedUrl = URLEncoder.encode(url, &quot;UTF-8&quot;); System.out.println(encodedUrl); // https%3A%2F%2Fwww.example.com%2Fsearch%3Fq%3DJava+%E7%BC%96%E7%A8%8B // URL解码 String decodedUrl = URLDecoder.decode(encodedUrl, &quot;UTF-8&quot;); System.out.println(decodedUrl); // https://www.example.com/search?q=Java 编程 org.apache.commons.codec.net.URLCodec：这是Apache Commons Codec库中提供的URL编解码类，与java.net.URLEncoder和java.net.URLDecoder类似 lang=Java import org.apache.commons.codec.net.URLCodec; // URL编码 String url = &quot;https://www.example.com/search?q=Java 编程&quot;; String encodedUrl = new URLCodec().encode(url); System.out.println(encodedUrl); // https%3A%2F%2Fwww.example.com%2Fsearch%3Fq%3DJava+%E7%BC%96%E7%A8%8B // URL解码 String decodedUrl = new URLCodec().decode(encodedUrl); System.out.println(decodedUrl); // https://www.example.com/search?q=Java 编程 C： C语言中没有标准库提供URL编解码函数，一般手动实现。 lang=C #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;ctype.h&gt; // URL编码函数 char *urlencode(const char *str) { const char *hex = &quot;0123456789ABCDEF&quot;; size_t len = strlen(str); char *buf = malloc(len * 3 + 1), *pbuf = buf; for (size_t i = 0; i &lt; len; i++) { if (isalnum(str[i]) || strchr(&quot;-_.~&quot;, str[i])) { *(pbuf++) = str[i]; } else if (str[i] == ' ') { *(pbuf++) = '+'; } else { *(pbuf++) = '%'; *(pbuf++) = hex[(unsigned char) str[i] &gt;&gt; 4]; *(pbuf++) = hex[(unsigned char) str[i] &amp; 15]; } } *pbuf = '\\0'; return buf; } // URL解码函数 char *urldecode(const char *str) { size_t len = strlen(str); char *buf = malloc(len + 1), *pbuf = buf; for (size_t i = 0; i &lt; len; i++) { if (str[i] == '+') { *(pbuf++) = ' '; } else if (str[i] == '%' &amp;&amp; isxdigit(str[i + 1]) &amp;&amp; isxdigit(str[i + 2])) { int ch; sscanf(&amp;str[i + 1], &quot;%02x&quot;, &amp;ch); *(pbuf++) = ch; i += 2; } else { *(pbuf++) = str[i]; } } *pbuf = '\\0'; return buf; } ","link":"https://hill-jiang.github.io/post/bao-wen-chuan-shu-zhong-de-url-bian-ma-wen-ti/"},{"title":"TLS 是如何导致报文长度膨胀的","content":"问题背景 最近 4G 充电桩的项目遇到一个问题，实测设备每天消耗的 4G 流量，远远超出理论计算的值 能够理解 TCP 头部、IP 头部、以太网头部等网络协议头导致报文长度增长，但从实际抓包情况来看，去掉这些网络协议头后的报文长度，也远超出原始明文长度： 设备端发送明文的心跳报文长度应该是 34 字节，为什么加密后实际发了 64+96 字节？云端的心跳报文回复长度应该是 28 字节，为什么加密后实际收到了 80 字节？ 为什么设备与云端交互时，总会发送两个报文，第一个报文的长度始终为 64 字节？ 为了搞清楚原因，决定从 TLS 协议入手进行分析 TLS 协议 SSL/TLS 是一种密码通信框架，他是世界上使用最广泛的密码通信方法。SSL/TLS 综合运用了密码学中的对称密码，消息认证码，公钥密码，数字签名，伪随机数生成器等，可以说是密码学中的集大成者 SSL(Secure Socket Layer)安全套接层，是 1994 年由 Netscape 公司设计的一套协议，并与 1995 年发布了 3.0 版本 TLS(Transport Layer Security)传输层安全是 IETF 在 SSL3.0 基础上设计的协议，实际上相当于 SSL 的后续版本 TLS 协议主要分为两层，底层是 TLS 记录协议，负责使用对称密码对消息进行加密 上层是 TLS 握手协议，负责在客户端和服务端商定密码算法和共享密钥 TLS 握手协议 从抓包来看，我们的设备和基础云进行 TLS 握手时，经历了以下阶段： Client Hello，客户端向服务器端发送一个 client hello 的消息，包含下面内容： 可用版本号、当前时间、客户端随机数、会话 ID、可用的密码套件清单、可用的压缩方式清单 Server Hello，服务器端收到 client hello 消息后，会向客户端返回一个 server hello 消息，包含如下内容： 使用的版本号、当前时间、服务器随机数、会话 ID、使用的密码套件、使用的压缩方式 可以看到，我们的设备和基础云协商的加密方式为 TLS_RSA_WITH_AES_128_CBC_SHA256，压缩方式无 Certificate，服务端发送自己的证书清单，因为证书可能是层级结构的，所以除了服务器自己的证书之外，还需要发送为服务器签名的证书 Server Hello Done，服务器端发送 server hello done 的消息告诉客户端自己的消息结束了 ![image-20220820163300335](TLS 是如何导致报文长度膨胀的.assets/image-20220820163300335.png) Client Key Exchange，公钥或者 RSA 模式情况下，客户端将根据客户端生成的随机数和服务器端生成的随机数，生成预备主密码，通过该公钥进行加密，返送给服务器端 Client Change Cipher Spec、Finish，客户端准备切换密码，表示后面的消息将会以前面协商过的密钥进行加密 Server Change Cipher Spec、Finish，服务端准备切换密码，表示后面的消息将会以前面协商过的密钥进行加密 TLS 记录协议 TLS 记录协议主要负责消息的压缩，加密及数据的认证 首先，消息被分割成多个较短的片段，然后分别对每个片段进行压缩，压缩算法需要与通信对象协商决定 接下来，经过压缩的片段会被加上消息认证码，这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。单项散列的函数的算法，以及消息认证码所使用的共享密钥都需要与通信对象协商决定 再接下来，经过压缩的片段再加上消息认证码会一起通过对称加密进行加密。加密使用 CBC 模式，CBC 模式的初始向量 IV 通过主密码生成，而对称密码的算法以及共享密码需要与通信对象协商决定 分析 从握手阶段的 ServerHello 报文，我们知道了此次通信无压缩、使用的加密方式为 TLS_RSA_WITH_AES_128_CBC_SHA256 这个加密方式实际上分为了好几个部分： RSA 表示使用 RSA 非对称加密来传输 AES 密钥 AES_128_CBC 表示应用数据使用 AES_128_CBC 模式来加密 SHA256 表示生成 MAC 的摘要算法使用 SHA256 RSA 个没啥好分析的，握手阶段就已经通过 RSA 进行了 AES 密钥传输 AES_128_CBC AES_128_CBC 是一种分组对称加密算法，即用同一组 key 进行明文和密文的转换，key 的长度为 128bit： 以 128bit 为一组，128bit=16Byte，意思就是明文的 16 字节为一组，对应加密后的 16 字节的密文 若最后剩余的明文不够 16 字节，需要进行填充，通常采用 PKCS7 进行填充。比如最后缺 3 个字节，则填充 3 个字节的 0x03；若最后缺 10 个字节，则填充 10 个字节的 0x0a 若明文正好是 16 个字节的整数倍，最后要再加入一个 16 字节 0x10 的组再进行加密 CBC 模式为：用初始向量和密钥加密第一组数据，然后把第一组数据加密后的密文重新赋值给 IV，然后进行第二组加密，循环进行直到结束 那么通过 AES_128_CBC 加密原始明文，得到的最终长度一定是 16 字节的整数倍，会导致 1-16 字节的膨胀（密文长度比明文长度大 1-16 字节） SHA256 对于任意长度的消息，SHA256 都会产生一个 256 位的哈希值，也就是 32 个字节。 在 TLS 记录协议中，对压缩后的消息片段进行 MAC 值的计算用的散列函数就是 SHA256，详细的 MAC 值计算方法不展开了，反正最终 MAC 长度是 32 字节。 加密数据长度 回到 TLS 记录协议上，TLS 1.2 记录协议中，报文经过 ASE_CBC 块加密后的完整组成结构如下： struct { opaque IV[SecurityParameters.record_iv_length]; block-ciphered struct { opaque content[TLSCompressed.length]; opaque MAC[SecurityParameters.mac_length]; uint8 padding[GenericBlockCipher.padding_length]; uint8 padding_length; }; } GenericBlockCipher 总长度 = 向量 IV 长度 + 明文压缩后的长度 + MAC 长度 + 填充长度 IV 是指 AES_CBC 加密是使用的初始向量，其长度为块加密的 block_size，在 AES 加密中为 16 字节 无压缩，压缩后长度为原始明文长度 MAC 长度为 32 字节 填充长度为 1-16 字节 再来看看设备发的心跳报文，原始明文为 34 字节，拆成了 64 字节和 96 字节两个包发送，猜测是这样的： 第一个包：length = 0（plainText） + 16（IV） + 32（MAC） + 16（padding） = 64 第二个包：length = 34（plainText） + 16（IV） + 32（MAC） + 14（padding） = 96 至于为什么设备端总会发送数据之前发送一个内容为空的包，就涉及到具体设备端实现了，参考资料中有提到 发送 fragment 长度为 0 的应用数据在进行流量分析时是有用的 云端回复的报文，原始明文为 28 字节，实际加密后为 80 字节： length = 28（plainText） + 16（IV） + 32（MAC） + 4（padding） = 80 至此，TLS 加密导致报文长度膨胀的原因基本弄清楚了 总结 设备和云端使用 TLS 加密协议进行通信，协商的通信方法是 TLS_RSA_WITH_AES_128_CBC_SHA256，因为块加密填充、计算消息认证码等原因导致加密后的消息长度原大于原始明文长度 设备在发送消息时，总会把先发送一个原始明文长度为 0 的消息，与设备端实现有关 具体到 4G 充电桩这个项目上，产品让步不再进行流量限制，后续如果有对使用流量极为敏感的设备，可从加密角度着手，优化通信方式和加密协议，减少 TLS 加密带来的报文长度膨胀 参考 一篇文章让你彻底弄懂 SSL/TLS 协议 - 知乎 (zhihu.com) 对加密算法 AES-128-CBC 的一些理解 - 简书 (jianshu.com) https://halfrost.com/https_record_layer/ ","link":"https://hill-jiang.github.io/post/tls-shi-ru-he-dao-zhi-bao-wen-chang-du-peng-zhang-de/"},{"title":"rtmp+flv服务搭建与基于python的flv流获取","content":"背景 最近有个项目，用户可以在微信小程序上直接预览监控点，而不用额外下载 APP 之前只能通过 APP 预览，基联 APP 的接口编写过模拟多路并行预览的工具 但是小程序的鉴权方式完全不一致，而且预览流程也完全不一样（基于 HTTP+FLV），针对小程序的预览模拟，需要编写另外的脚本工具 另外也立项了 RTMP 推流的需求，为了提前了解下相关协议，也为了方便脚本调试，尝试在本地搭建了相关服务并进行了脚本模拟拉流测试 本地服务搭建 本地搭建的推流、拉流框架如下： 启动 nginx，开启 RTMP 服务，配置 HTTP 开启 FLV 服务 通过 ffmpeg 将视频文件转码推流到 RTMP 服务 通过 VLC 等拉流工具，使用 RTMP 协议或 FLV 协议进行拉流 nginx-http-flv-module 源码编译 nginx 本身是不支持流媒体功能的，开发者们为其添加了额外的流媒体功能，比如开源的 nginx-http-flv-module 但需要重新编译 Windows 上源码编译 nginx 环境配置很麻烦，直接找编译好的包，解压就能使用 万恶的 CSDN 上倒是有很多，但都要付费下载 经过不懈努力终于在 github 上找到了一个编译好的包：https://github.com/chen-jia-hao/nginx-win-httpflv-1.19.0 nginx 配置文件修改 修改 conf/nginx.conf worker_processes 1; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #error_log logs/error.log debug; #pid logs/nginx.pid; events { worker_connections 1024; } rtmp_auto_push on; rtmp_auto_push_reconnect 1s; rtmp_socket_dir temp; # 添加RTMP服务 rtmp { server { listen 1935; # 监听端口 chunk_size 4000; application live { live on; gop_cache on; # GOP缓存，on时延迟高，但第一帧画面加载快。off时正好相反，延迟低，第一帧加载略慢。 } } } # HTTP服务 http { include mime.types; default_type application/octet-stream; #access_log logs/access.log main; server { listen 80; # 监听端口 location / { add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; if ($request_method = 'OPTIONS') { return 204; } root html; } location /live { flv_live on; #打开HTTP播放FLV直播流功能 chunked_transfer_encoding on; #支持'Transfer-Encoding: chunked'方式回复 add_header 'Access-Control-Allow-Origin' '*'; #添加额外的HTTP头 add_header 'Access-Control-Allow-Credentials' 'true'; #添加额外的HTTP头 } location /stat.xsl { root html; } location /stat { rtmp_stat all; rtmp_stat_stylesheet stat.xsl; } location /control { rtmp_control all; #rtmp控制模块的配置 } } } 启动 nginx start nginx -c conf/nginx.conf ffmpeg 推流 ffmpeg -stream_loop -1 -re -i 诸葛亮王朗.mp4 -vcodec libx264 -acodec aac -f flv rtmp://localhost:1935/live/123 VLC 拉流 VLC 媒体-打开网络串流： http://localhost/live?port=1935&amp;app=live&amp;stream=123 随后即可播放： Python 获取 FLV 视频流 因为目的是模拟多路并发预览，考虑用 Python 脚本实现多路并行获取 FLV 视频流，调研对比了多种实现方案 基于 OpenCV 库 OpenCV 库提供了简单的 API，可直接获取网络视频流保存到本地文件： import cv2 def save_video(flv_url): cap = cv2.VideoCapture(flv_url) fps = cap.get(cv2.CAP_PROP_FPS) size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))) fourcc = cv2.VideoWriter_fourcc('F', 'L', 'V', '1') video_file = 'video/test.flv' out_video = cv2.VideoWriter(video_file, fourcc, fps, size) rval, frame = cap.read() while rval: out_video.write(frame) rval, frame = cap.read() cv2.waitKey(1) cap.release() out_video.release() 实测保存的本地文件可以用 VLC 或 ffplay 直接播放 但我的需求是模拟多路并发预览，OpenCV 库提供的获取流方法是阻塞式的，没法套用已有的 async 协程框架，想要实现多并发得用多线程等方式实现 为了复用之前的框架，同时也为了更深入地理解 FLV 协议，还是决定用 asyncio 直接建立 Socket 连接试试 基于 Socket 连接 HTTP-FLV，即将音视频数据封装成 FLV，然后通过 HTTP 协议传输给客户端。 建立连接后，需要发送 FLV 协议规定的 HTTP 请求头，比如用 VLC 拉流，抓包看到建立 TCP 连接后，发送的 HTTP 请求及响应如下： 因为服务器并不知道流的长度，所以响应的 HTTP 头并没有携带 Content-Length 字段，而是携带 Transfer-Encoding: chunked 字段，这样客户端就会一直接收数据了 编写脚本用 asyncio 直接建立 Socket 连接，获取数据保存到本地文件： import asyncio import async_timeout from urllib.parse import urlparse async def save_video(flv_url): video_file = &quot;video/test.flv&quot; if os.path.exists(video_file): os.remove(video_file) flv_hostname = urlparse(flv_url).hostname flv_port = &quot;80&quot; flv_path = urlparse(flv_url).path flv_query = urlparse(flv_url).query try: with async_timeout.timeout(20): reader, writer = await asyncio.open_connection(flv_hostname, flv_port) print(&quot;Flv Server Connected&quot;) except asyncio.TimeoutError: print(&quot;Connection Timeout!&quot;) except ConnectionError: print(&quot;Connection Failed!&quot;) try: header = &quot;&quot;&quot;GET {}?{} HTTP/1.1 Host: {} Accept: */* Accept-Language: zh_CN User-Agent: VLC/3.0.8 LibVLC/3.0.8 Range: bytes=0- &quot;&quot;&quot;.format(flv_path, flv_query, flv_hostname) writer.write(header.encode()) await writer.drain() recv_data = await reader.read(1024) recv_header = recv_data.split(b'\\r\\n\\r\\n')[0] print(recv_header.decode()) if 'HTTP/1.1 200 OK' in recv_header.decode(): print(&quot;Video Get Success&quot;) if recv_data.split(b'\\r\\n\\r\\n')[1]: flv_header_index = recv_data.split(b'\\r\\n\\r\\n')[1].find(b'\\x46\\x4C\\x56') flv_header = recv_data.split(b'\\r\\n\\r\\n')[1][flv_header_index:] with open(video_file, 'wb') as fd: fd.write(flv_header) else: recv_data = await reader.read(1024) flv_header_index = recv_data.find(b'\\x46\\x4C\\x56') flv_header = recv_data[flv_header_index:] with open(video_file, 'wb') as fd: fd.write(flv_header) while True: recv_data = await reader.read(1024) with open(video_file, 'wb') as fd: fd.write(recv_data) except ConnectionError: print(&quot;Connection Failed!&quot;) 其中 b'\\x46\\x4C\\x56' 对应 FLV，即 FLV 头部，从服务器响应的 FLV 头部开始的数据保存到文件中，但是保存下来的文件却无法通过 ffplay 或 VLC 播放 对比保存的文件内容，与抓包结果一致： 再对比通过 OpenCV 保存的文件，虽然可以播放，但是与抓包结果的 FLV 头部却不一样： 说明 OpenCV 在获取视频流数据、保存到文件的时候就对头部做了一些处理，让其可以正常播放 而直接把通过 Socket 获取到的二进制数据保存到文件，其 FLV 头部并不是合法的格式，所以无法直接播放 基于 requests 查找资料的时候发现，基于 requests 库可以直接用 get 方法获取 HTTP-FLV 数据，同样可以保存到文件： import requests def save_video_requests(flv_url): video_file = &quot;video/test_requests.flv&quot; if os.path.exists(video_file): os.remove(video_file) chunk_size = 1024 response = requests.get(flv_url, stream=True, verify=False) with open(video_file, 'wb') as file: for data in response.iter_content(chunk_size = chunk_size): file.write(data) file.flush() 尝试了一下发现此方法保存的文件同样可以直接播放，对比抓包结果与文件内容如下： 发现好像保存的文件就是去掉了抓包结果中的一些换行符（0d0a），部分换行符前面还有一些数据，看来也是保存的时候底层做了一些处理。 其实换行符和部分换行符前面的数据是 HTTP 分块传输编码规则导致的： 每个分块包含两个部分，长度头和数据块； 长度头是以 CRLF（回车换行，即 \\r\\n）结尾的一行明文，用 16 进制数字表示长度； 数据块紧跟在长度头后，最后也用 CRLF 结尾，但数据不包含 CRLF； 最后用一个长度为 0 的块表示结束，即 0\\r\\n\\r\\n。 所以我们只要在保存数据的时候，只保存 chunked data，把 length 和换行符都过滤掉就可以了 这原理看起来简单，但真要直接处理二进制数据还比较复杂 不过既然 requests 可以实现，那协程的 aiohttp 应该也可以吧 基于 aiohttp import aiohttp async def save_video_aiohttp(flv_url): video_file = &quot;video/test_aiohttp.flv&quot; if os.path.exists(video_file): os.remove(video_file) chunk_size = 1024 conn = aiohttp.TCPConnector() async with aiohttp.ClientSession(connector=conn) as session: async with session.get(flv_url) as response: with open(video_file, 'wb') as file: while True: data = await response.content.read(chunk_size) if not data: break file.write(data) file.flush() 测试能通过 ffplay 和 VLC 正常播放，aiohttp 套入协程框架也很方便，最终就决定用这种方式了 参考 https://www.cnblogs.com/hhmm99/p/16050844.html https://github.com/winshining/nginx-http-flv-module https://github.com/chen-jia-hao/nginx-win-httpflv-1.19.0 https://www.cnblogs.com/vczf/p/14813438.html https://blog.csdn.net/Enderman_xiaohei/article/details/102626855 ","link":"https://hill-jiang.github.io/post/rtmpflv-fu-wu-da-jian-yu-ji-yu-python-de-flv-liu-huo-qu/"},{"title":"博客收集","content":" 科技爱好者周刊 HelloGitHub Python 潮流周刊 测试之家 ","link":"https://hill-jiang.github.io/post/博客收集/"},{"title":"Python 字典的浅拷贝与深拷贝","content":"最近在写 Python 脚本的时候用到了字典的拷贝，踩了一个坑，在此记录一下 对于 Python 字典，如果直接用=赋值，修改一个字典会同时修改另一个： Python 3.7.4 (tags/v3.7.4:e09359112e, Jul 8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; dict_1 = {'1': 1} &gt;&gt;&gt; dict_2 = dict_1 &gt;&gt;&gt; dict_2['1'] = 2 &gt;&gt;&gt; dict_2 {'1': 2} &gt;&gt;&gt; dict_1 {'1': 2} 一般使用copy()方法进行拷贝，在修改拷贝后的字典时，不会影响原来的字典： Python 3.7.4 (tags/v3.7.4:e09359112e, Jul 8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; dict_1 = {'1': 1} &gt;&gt;&gt; dict_2 = dict_1.copy() &gt;&gt;&gt; dict_2['1'] = 2 &gt;&gt;&gt; dict_2 {'1': 2} &gt;&gt;&gt; dict_1 {'1': 1} 但是，当字典超过一层时，修改copy()后的键值时会同样修改原字典的键值： &gt;&gt;&gt; dict_3 = {'1': {'1': 3}} &gt;&gt;&gt; dict_4 = dict_3.copy() &gt;&gt;&gt; dict_4['1']['1'] = 4 &gt;&gt;&gt; dict_4 {'1': {'1': 4}} &gt;&gt;&gt; dict_3 {'1': {'1': 4}} 后来网上查了一下才知道，Python 字典的拷贝分为浅拷贝和深拷贝 copy()为浅拷贝，只拷贝字典的父级目录；而deepcopy()为深拷贝，会将整个字典全都拷贝 使用deepcopy()需要导入copy模块 Python 3.7.4 (tags/v3.7.4:e09359112e, Jul 8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import copy &gt;&gt;&gt; dict_3 = {'1': {'1': 3}} &gt;&gt;&gt; dict_4 = copy.deepcopy(dict_3) &gt;&gt;&gt; dict_4['1']['1'] = 4 &gt;&gt;&gt; dict_4 {'1': {'1': 4}} &gt;&gt;&gt; dict_3 {'1': {'1': 3}} ","link":"https://hill-jiang.github.io/post/Python字典的浅拷贝与深拷贝/"},{"title":"Python 进程间通信初探","content":"需求背景 最近在写商用云平台的虚拟充电桩设备，本来想着在之前虚拟 IPC 的基础上修改一些类型字段、加一些特定的适配协议和逻辑就可以了，后来发现还是有些不足的地方。 现有的虚拟设备是基于 Python locust 框架写的，消息上报实现方案是预定义好上报的周期，比如每分钟上报一次，在 locust 中预构建一个子任务，每分钟调用一次消息上报接口。这样的方法不足是，没法模拟真实设备的场景，随时触发任意类型的消息进行上报。另外，如果在运行过程中，需要手动修改某些虚拟设备的配置信息，也无法实现，只能停止后修改再重新运行。 换句话说，现有的虚拟设备工具，所有配置和逻辑都只能在运行前就预定义好，一旦运行之后就无法再介入修改了。 需要实现的优化目标是，在虚拟设备工具运行过程中，外部触发某个条件，能够直接影响内部正在运行的任务，这就涉及到了 Python 进程间的通信。 概念 进程是操作系统分配和调度系统资源（CPU、内存）的基本单位。进程之间是相互独立的，每启动一个新的进程相当于把数据进行了一次克隆，子进程里的数据修改无法影响到主进程中的数据，不同子进程之间的数据也不能直接共享，这是多进程在使用中与多线程最明显的区别。 常用的进程间通信方法有很多： 信号量（ semaphore ） 信号 （ signal ） 管道（ pipe ） 消息队列（ message queue ） 共享内存（ shared memory ） 套接字（ socket ） 文件 （ file ） 具体到 Python，以上提到的进程间通信方法也都有对应实现。 信号量（semaphore） 信号量是一个共享资源访问者的计数器，可以用来控制多个进程对共享资源的并发访问数。它常作为一种锁机制，防止指定数量的进程正在访问共享资源时，其他进程也访问该资源。每次有一个进程获得信号量时，计数器 -1，若计数器为 0 时，其他进程就停止访问信号量，一直阻塞直到其他进程释放信号量。 示例： import os import time from multiprocessing import Process, Semaphore def handle(sem): sem.acquire() print(f&quot;{int(time.time())}, {os.getpid()} 开始处理事件&quot;) time.sleep(1) print(f&quot;{int(time.time())}, {os.getpid()} 结束处理事件&quot;) sem.release() if __name__ == '__main__': sem = Semaphore(4) for i in range(5): p = Process(target=handle, args=(sem,)) p.start() 运行结果： 1653378426, 216 开始处理事件 1653378426, 3888 开始处理事件 1653378426, 9064 开始处理事件 1653378426, 13968 开始处理事件 1653378427, 216 结束处理事件 1653378427, 17160 开始处理事件 1653378427, 9064 结束处理事件 1653378427, 3888 结束处理事件 1653378427, 13968 结束处理事件 1653378428, 17160 结束处理事件 可以看到，同时最多只有 4 个进程处理事件，当一个进程的事件处理结束释放信号量后，第 5 个进程才能开始处理事件。 信号量常用于控制某共享资源的多进程并发访问者数量，并不适用于进程之间传输具体数据。 信号（sginal） 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 进程间信号最先出现于 UNIX 系统，每个信号都有自己的系统调用，后续修改为统一的 signal() 与 kill() 调用。最初的进程间信号系统是异步的，而且没有队列的概念，即不同信号间很容易产生冲突，导致应用程序来不及处理前一个信号。POSIX 规范后来改进了这一设计，另外规定了实时信号，靠队列的方式避免了信号冲突的问题。 为了统一各系统下进程间信号与其整数的统一，POSIX 规范规定了 19 个信号及其对应整数与行为： Signal Value Action Comment ─────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 SIGCHLD 20,17,18 Ign Child stopped or terminated SIGCONT 19,18,25 Cont Continue if stopped SIGSTOP 17,19,23 Stop Stop process SIGTSTP 18,20,24 Stop Stop typed at terminal SIGTTIN 21,21,26 Stop Terminal input for background process SIGTTOU 22,22,27 Stop Terminal output for background process 在 Linux/UNIX 下，由于 SIGINT 与 SIGTSTP 信号较为常用，这两个信号可以分别使用 Ctrl+C 与 Ctrl+Z 快捷键触发，Windows 支持前者，但不支持后者。此外在 Linux/UNIX 下还有一个不常用的 Ctrl+\\ 快捷键，用于发送 SIGQUIT 信号。 首先介绍一个简单的方式，即异常捕获。Python 脚本运行过程中按下中断键（如 Ctrl+C）会触发一个 KeyboardInterrupt 异常，我们只要在需要处理中断的代码段外使用 try...except... 将其包裹起来即可，如下： try: # Some code except KeyboardInterrupt: # Another code 使用上文异常捕获的方式存在若干不足。一方面，对于一个庞大的系统来说，可能在不同的执行阶段对于退出有不同的处理方式；另一方面，尽管使用 Ctrl+C 热键触发 SIGINT 中断是最常见的方式，但并非所有 SIGINT 信号都是通过热键触发，也并非所有信号都是 SIGINT。Python 为了实现信号的安装，引入了 signal 模块。下文以 SIGINT 与 SIGTERM 为例，简述该模块的使用。 import signal def bye(signum, frame): print(&quot;Bye bye&quot;) exit(0) signal.signal(signal.SIGINT, bye) signal.signal(signal.SIGTERM, bye) while True: pass 当执行过程中按下 Ctrl+C 或在其他终端窗口中输入 kill -2 [pid]（2 的含义见上表）时，可以看到 bye(signum, frame) 函数被调用，并成功退出。 与信号量类似，信号仅用于进程间传递特定信号，并不适用于数据传输。 管道（pipe） 管道常用来在两个进程间进行通信，两个进程分别位于管道的两端。 Python multiprocessing 模块的 Pipe 方法返回（conn1， conn2）代表一个管道的两个端。Pipe 方法有 duplex 参数，如果 duplex 参数为 True（默认值），那么这个参数是全双工模式，也就是说 conn1 和 conn2 均可收发。若 duplex 为 False，conn1 只负责接收消息，conn2 只负责发送消息。send 和 recv 方法分别是发送和接受消息的方法。例如，在全双工模式下，可以调用 conn1.send 发送消息，conn1.recv 接收消息。如果没有消息可接收，recv 方法会一直阻塞。如果管道已经被关闭，那么 recv 方法会抛出 EOFError。 示例：创建两个进程，一个子进程通过 Pipe 发送数据，一个子进程通过 Pipe 接收数据。 from multiprocessing import Pipe, Process import os import time def proc_send(pipe, data): for d in data: print(f&quot;{os.getpid()} send: {d}&quot;) pipe.send(d) time.sleep(1) def proc_recv(pipe): while True: print(f&quot;{os.getpid()} recv: {pipe.recv()}&quot;) time.sleep(1) if __name__ == '__main__': pipe = Pipe() p1 = Process(target=proc_send, args=(pipe[0], [i for i in range(5)])) p2 = Process(target=proc_recv, args=(pipe[1],)) p1.start() p2.start() 输出如下： 16132 send: 0 16596 recv: 0 16132 send: 1 16596 recv: 1 16132 send: 2 16596 recv: 2 16132 send: 3 16596 recv: 3 16132 send: 4 16596 recv: 4 消息队列（message queue） 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 Python Queue 是多进程安全的队列，可以使用 Queue 实现多进程之间的数据传递。 put 方法用以插入数据到队列中， put 方法还有两个可选参数： blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.full 异常。 get 方法可以从队列读取并且删除一个元素。同样， get 方法有两个可选参数： blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果 blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。 示例：创建两个进程，一个子进程通过 Queue 发送数据，一个子进程通过 Queue 接收数据。 from multiprocessing import Queue, Process import time import os def send(q, data): for d in data: print(f&quot;{os.getpid()} send: {d}&quot;) q.put(d) time.sleep(1) def recv(q): while True: if not q.empty(): print(f&quot;{os.getpid()} recv: {q.get()}&quot;) time.sleep(1) else: continue if __name__ == '__main__': q = Queue() p1 = Process(target=send, args=(q, [i for i in range(5)])) p2 = Process(target=recv, args=(q,)) p1.start() p2.start() 输出如下： 16564 send: 0 9984 recv: 0 16564 send: 1 9984 recv: 1 16564 send: 2 9984 recv: 2 16564 send: 3 9984 recv: 3 16564 send: 4 9984 recv: 4 管道和消息队列可用于进程间传输数据，但每次传递的数据大小受限，效率偏低。 共享内存（shared memory） 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的进程间通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。 Python 标准库中实现共享内存通信的工具有 mmap，但是该库只能用于基本类型，且需要预先分配存储空间，对于自定义类型的对象使用起来有诸多不便。 比较好用的第三方工具有 apache 开源的 pyarrow，不需要预先定义存储空间且任意可序列化的对象均可存入共享内存。但使用时需要注意：pyarrow 反序列化的对象为只读对象不可修改其值，想要修改对象可先通过对象 copy。 Python 3.8 及以上版本支持 multiprocessing.shared_memory，一般来说，进程被限制只能访问属于自己进程空间的内存，但是共享内存允许跨进程共享数据，从而避免通过进程间发送消息的形式传递数据。相比通过磁盘、套接字或者其他要求序列化、反序列化和复制数据的共享形式，直接通过内存共享数据拥有更出色性能。 shared_memory 的使用也非常简单，只需要定义一个 ShareableList，就可以在多个进程之间访问同一对象。 ShareableList 提供一个可修改的类 list 对象，其中所有值都存放在共享内存块中。这限制了可被存储在其中的值只能是 int, float, bool, str （每条数据小于 10M）, bytes （每条数据小于 10M）以及 None 这些内置类型。它另一个显著区别于内置 list 类型的地方在于它的长度无法修改（比如，没有 append, insert 等操作）且不支持通过切片操作动态创建新的 ShareableList 实例。 a.py： from multiprocessing import shared_memory shared_list = shared_memory.ShareableList([None, 1, &quot;haha&quot;, False, b'123'], name=&quot;test&quot;) while True: pass b.py： from multiprocessing import shared_memory data = shared_memory.ShareableList(name=&quot;test&quot;) for d in data: print(d) 先运行 a.py，再运行 b.py，得到的运行结果为： None 1 haha False b'123' 相比于管道和消息队列，共享内存具有更高的性能和更大的数据传输量。 套接字（socket） socket 无疑是通信使用最为广泛的方式了，它不但能跨进程还能跨网络。在两个进程中创建 socket 连接，就能实现相互通信。 基本的 socket 就是基于以太网的套接字，也是最常用的。server 端先创建一个套接字，绑定一个本地端口，等待 client 连接；client 也创建一个套接字，直接连接到 server 端就可以正常进行通信了。不过，传输的数据必须为 byte 类型。 server.py： import socket sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((&quot;localhost&quot;, 2333)) sock.listen() c, addr = sock.accept() print(f&quot;Accept connect from {addr}&quot;) while True: data = c.recv(1024).decode() if data: print(data) c.send(f&quot;{data} Reply&quot;.encode()) client.py： import socket sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect((&quot;localhost&quot;, 2333)) sock.send(b&quot;Data&quot;) print(sock.recv(1024).decode()) 先运行 server.py，再运行 client.py，server 端输出： Accept connect from ('127.0.0.1', 52410) Data client 端输出： Data Reply Unix domain socket： 当同一个机器的多个进程使用普通套接字进行通信时，需要经过网络协议栈，这非常浪费，因为同一个机器根本没有必要走网络。所以 Unix 提供了一个套接字的特殊版本，它使用和套接字一摸一样的 api，但是地址不再是网络端口，而是文件。相当于我们通过某个特殊文件来进行套接字通信，不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程。 具体实现上，只需要在创建套接字的时候指定创建方式为 socket.AF_UNIX 即可： server_addr = &quot;./tmp_sock&quot; sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) sock.bind(server_addr) 不过 Unix 域套接字仅适用于 Unix 系统，在 Windows 系统下还是老实用以太网套接字绑定 localhost 吧。 文件（file） 使用文件进行通信是最简单的一种通信方式，一个进程将结果输出到临时文件，另一个进程从文件中读出来。 这个实现很简单，就不做示例了，不过感觉效率很低，频繁读写文件可能还存在同步和性能开销问题，不是很推荐使用。 总结 本文对 Python 中的各类进程间通信实现方案进行了介绍。 使用总结： 仅进程同步不涉及数据传输，可以使用信号、信号量； 若进程间需要传递少量数据，可以使用管道、消息队列； 若进程间需要传递大量数据，最佳方式是使用共享内存，这样减少数据拷贝、传输的时间内存代价； 跨主机的进程间通信（RPC）可以使用 socket 通信，但 socket 同样可用于相同主机不同进程间的通信。 对各类进程间通信方案，只是做了简单的试用，并未涉及实现原理、冲突处理、锁机制、性能表现等，将在实际开发使用过程中继续深入了解。 参考 Python 实现多进程间通信的方法总结_tyhj_sf 的博客-CSDN 博客_python 多进程通信方式 一文读懂 Python 进程间通信的几种方式 - 知乎 (zhihu.com) 6. python 多进程信号量－Semaphore - 简书 (jianshu.com) 进程间信号简述及使用 Python 响应进程间信号 - 知乎 (zhihu.com) python 多进程-进阶-进程间通信之 Pipe_运维家的博客-CSDN 博客_python 多进程 pipe Python 多进程（三）——进程间通信 &amp; Queue 队列_hxxjxw 的博客-CSDN 博客_python 进程间通信 queue multiprocessing.shared_memory --- 可从进程直接访问的共享内存 — Python 3.10.4 文档 深入 Python 进程间通信原理——图文版 - 知乎 (zhihu.com) Python 网络编程 | 菜鸟教程 (runoob.com) ","link":"https://hill-jiang.github.io/post/Python 进程间通信/"},{"title":"Linux 进程相关命令简介","content":"前言 日常工作中我们经常通过串口管理设备或是查看某些信息，而串口连接的设备系统本质上就是在嵌入式设备上运行的 Linux 系统。 本文介绍了一些 Linux 系统进程相关的命令使用方法以及含义，需要使用时可以快速查阅。 1 什么是进程 进程与程序 程序（Program）：通常为二进制程序，放置在存储媒介中（如硬盘、光盘、软盘、磁带等），以物理文件的形式存在。 进程（Process）：程序被触发后，执行者的权限与属性、程序的代码与所需数据等都会被加载到内存中，操作系统给予这个内存中的单元一个标识符（PID），可以说进程就是一个在运行中的程序。 服务：常驻在内存中的进程 常驻在内存中的进程通常都是负责一些系统所提供的功能以服务用户的各项任务，因此这些常驻进程就会被我们成为：服务（deamon）。 系统的服务非常多，主要分成系统本身所需要的服务（例如 crond、atd 以及 rsyslogd 等）和负责网络连接的服务（例如 apache、named、postfix、vsftpd 等）。 网络服务被执行后，会启动一个负责网络监听的端口（port），以提供外部客户端（client）的连接请求。 Linux 系统为了让用户简单判断一个进程是否为 deamon，在一般的 deamon 类型的进程文件名后面都会加上 d，例如 httpd、vsftpd 等 2 进程管理 查看进程 ps: 输出某个时间点的进程运行情况 选项与参数： -A: 所有的进程均显示出来，与-e 效果相同 -a: 不显示与终端有关的进程 -u: 有效使用者（effective user)相关的进程 -x: 通常这个参数与 u 一起使用，可列出比较完整的信息 输出格式规划： -l: 较长，较详细地将该 PID 的信息列出 -j: 任务的格式（jobs format） -f: 做一个更为完整的输出 仅查看自己的 bash 相关的进程：ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD 4 S 0 2034 1866 0 80 0 - 18201 poll_s pts/0 00:00:00 sudo 4 R 0 2045 2034 0 80 0 - 9008 - pts/0 00:00:00 ps F：代表这个进程标识（process flags），说明这个进程的权限，常见号码有： 4，表示此进程的权限为 root 1，表示此子进程仅执行复制（fork）而没有实际执行（exec） S：代表这个进程的状态（STAT），主要状态有： R(Running)，该进程正在进行中 S(Sleep)，该进程目前正在睡眠状态（idle），但可以被唤醒（signal） D，不可被唤醒的睡眠状态，通常这个进程可能在等待 I/O 的情况 T，停止状态，可能是在任务控制（后台暂停）或跟踪（traced）状态 Z(Zombie)，僵尸状态，进程已经终止但却无法被删除至内存外 UID/PID/PPID：代表此进程被该 UID 拥有/此进程的 PID 号码/此进程的父进程 PID 号码 C：代表 CPU 使用率，单位为百分比 PRI/NI：Priority/Nice 的缩写，代表此进程被 CPU 所执行的优先级，数值越小代表该进程越快被 CPU 执行 ADDR/SZ/WCHAN：都与内存有关，ADDR 是 kernel function，指出该进程在内存的哪个部分，如果是个 running 的进程，一般会显示【-】；SZ 代表此进程占用了多少内存；WCHAN 表示目前进程是否在运行，若为【-】表示正在运行中 TTY：登录者的终端位置，若为远程登录则使用动态终端接口名称（pts/n） TIME：使用的 CPU 时间，此进程实际花费 CPU 运行的时间 CMD：表示造成此进程触发的命令 查看系统所有进程：ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.4 159708 8140 ? Ss 22:49 0:01 /sbin/init aut root 2 0.0 0.0 0 0 ? S 22:49 0:00 [kthreadd] ...中间省略... jss 2119 0.0 0.1 46788 3628 pts/0 R+ 23:01 0:00 ps aux USER：该进程属于所属用户账号 PID：该进程的进程 ID %CPU：该进程占用的 CPU 百分比 %MEM：该进程占用的物理内存白分别 VSZ：该进程使用的虚拟内存量（KB） RSS：该进程占用的固定的内存（KB） TTY：该进程是在哪个终端上面运行，若与终端无关则显示【?】tty1-tty6 时本机上的登录进程，若为 pts/0 等，则表示是有网络连接进入主机的进程 STAT：该进程目前的状态，与 ps -l 的 S 标识相同（R/S/T/Z） START：该进程被触发启动的时间 TIME：该进程实际使用 CPU 运行的时间 COMMAND：造成该进程触发的命令 一般来说，ps aux会依照 PID 的顺序来排序显示。 top：动态查看进程的变化 top [-d 数字] | top [-bnp] 选项与参数： -d：后面可以接秒数，就是整个进程界面更新的秒数，默认是 5 秒 -b：以批量的方式执行 top，还有更多的参数可以使用，通常会搭配数据流重定向来将批量的结果输出为文件 -n：与-b 搭配，表示需要执行几次 top 的输出结果 -p：指定某些 PID 执行查看监测 在 top 执行过程中可以使用的命令： ?：显示在 top 中可以输入的按键命令 P：以 CPU 的使用排序显示 M：以内存的使用排序显示 N：以 PID 使用排序 T：由该进程使用的 CPU 时间累积（TIME+）排序 k：给予某个 PID 一个信号（signal） r：给予某个 PID 重新制定一个 nice 值 q：退出 top 与 ps 的静态结果输出不同，top 这个命令可以持续监测整个系统的进程任务状态。在默认情况下，每次更新进程资源的时间为 5 秒，可以使用-d 参数修改。 top - 14:47:33 up 3:50, 2 users, load average: 0.00, 0.00, 0.00 Tasks: 206 total, 1 running, 141 sleeping, 1 stopped, 0 zombie %Cpu(s): 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 2017460 total, 1054752 free, 502000 used, 460708 buff/cache KiB Swap: 969960 total, 969960 free, 0 used. 1358900 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 293 root 20 0 0 0 0 S 0.3 0.0 0:00.03 jbd2/sda1-8 1 root 20 0 159700 8856 6644 S 0.0 0.4 0:01.36 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H 6 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq ...（以下省略） 第一行（top...）： 目前的时间 开机到目前为止所经过的时间 已经登录系统的用户人数 系统在 1、5、15 分钟的平均负载。代表的是 1、5、15 分钟，系统平均要负责运行几个进程的意思。数值越小代表系统约闲置，若高于 1 则需要注意系统进程是否过于频繁 第二行（Tasks...）： 目前进程总量 Running、Sleeping、Stopped、Zombie 状态的进程数量 第三行（%Cpus...）：CPU 的整体负载，需要特别注意 wa 项，该项代表 IO wait，通常变慢可能都是 I/O 产生的问题 第四行与第五行：表示目前的物理内存与虚拟内存的使用情况 第六行及以下：显示进程状态 PID：每个进程的 PID USER：该进程所属的用户 PR：Priority，进程的优先执行顺序，越小则越早被执行 NI：Nice，与 Priority 有关，越小则越早被执行 %CPU：CPU 的使用率 %MEM：MEM 的使用率 TIME+：CPU 使用时间的累加 top 默认使用 CPU 的使用率作为排序依据，可使用【M】按使用内存排序，使用【N】以 PID 排序。 若想要将 top 的结果输出为文件，可以使用：top -b -n 2 &gt; /tmp/top.txt，上述命令将 top 的信息执行 2 次，然后将结果输出到/tmp/top.txt文件中 pstree pstree [-A|U] [-up] 选项与参数： -A：各进程树之间的连接以 ASCII 字符来连接 -U：各进程树之间的连接以 Unicode 的字符来连接 -p：同时列出每个进程的 PID -u：同时列出每个进程的所属账号名称 pstree 命令用于查找进程之间的相关性。 ps -up systemd(1)─┬─ModemManager(479)─┬─{ModemManager}(496) │ └─{ModemManager}(499) ├─NetworkManager(488)─┬─dhclient(634) │ ├─{NetworkManager}(563) ...（中间省略） ├─vmtoolsd(1097) ├─vmtoolsd(1664,jss)───{vmtoolsd}(1793) ├─vmware-vmblock-(1074)─┬─{vmware-vmblock-}(1075) │ └─{vmware-vmblock-}(1076) ├─whoopsie(1140,whoopsie)─┬─{whoopsie}(1154) │ └─{whoopsie}(1159) └─wpa_supplicant(487) 从运行结果可知，所有的进程都是依附在 systemd（PID 为 1）这个进程下的，因为它是由 Linux 内核所主动调用的第一个进程。 如果子进程挂掉或者总是 kill 不掉子进程，就用 pstree 找到它的父进程看看。 管理进程 进程之间时可以互相控制的。举例来说，你可以关闭、重新启动服务器软件，服务器软件本身是个进程，你既然可以让它关闭 i 或启动，当然就可以控制该进程。进程是如何互相管理的呢？其实是通过给予该进程一个信号（signal）去告知该进程你想要让它做什么。 使用kill -l命令可以查询所有的信号代号和名称，主要的信号如下： 代号 名称 内容 1 SIGHUP 启动被终止的进程，可让该 IPD 重新读取自己的配置文件，类似重启 2 SIGINT 相当于用键盘输入[ctrl]-c 来中断一个进程的运行 9 SIGKILL 代表强制中断一个进程的执行，如果该进程执行到一半，那么尚未完成的部分可能会有“半成品”产生 15 SIGTERM 以正常的方式结束进程来终止该进程。由于是正常的终止，所以后续的操作会将它完成。 19 SIGSTOP 相当于用键盘输入[ctrl]-z 来暂停一个进程的运行 例 1：重启 rsyslogd 进程 kill -SIGUP $(ps aux | grep 'rsyslogd' | grep -v 'grep' | awk '{print $2}') 例 2：强制终止所有以 httpd 启动的进程 killall -9 httpd 要删除某进程，可以使用 PID 或是启动该进程的命令名称，而如果要删除某个服务，最简单的方法是利用 killall，将系统中所有以某个命令名称启动的进程全部删除。 进程的执行顺序 Priority 与 Nice 值 jss@ubuntu:~$ ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD 0 S 1000 1792 1791 0 80 0 - 7439 wait pts/0 00:00:00 bash 0 R 1000 31783 1792 0 80 0 - 9008 - pts/0 00:00:00 ps PRI 和 NI 都用来表示进程的运行优先级，那么它们有什么不同呢？ PRI 值由内核动态调整，用户无法直接调整 PRI 值。但是用户可以设置 NI 值，一般来说，PRI 与 NI 的相关性为： PRI(new) = PRI(old) + NI 需要注意的是，并不一定原本 PRI 的值为 50，我们给予一个 NI=5，就会让 PRI 变成 55。因为 PRI 是系统动态决定的，虽然 NI 值可以影响 PRI，但最终的 PRI 仍是要经过系统分析后才决定的。另外，NI 值是有正负的，当 NI 值为负时，就可以让该进程优先级提升。 NI 值可调整的范围为-20~19 root 可随意调整自己或它人的 NI，且范围为-20~19 一般用户仅可调整自己进程的 NI，且范围为 0~19 一般用户仅可将 NI 值越调越高，而不能降低 给予新执行的命令 NI 值：nice [-n number] command 为已存在的进程调整 NI 值：renice [number] PID NI 值可以在父进程和子进程之间传递。 查看系统资源信息 free：查看内存使用情况 free [-b|-k|-m|-g|-h] [-t] [-s N -c N] 选项与参数： -b：直接输入 free 时，显示的单位是 KBytes，可以使用 b(Bytes)、m(MBytes)、k(KBytes)、g(Gbytes)来指定单位，也可以直接让系统自己指定单位(-h) -t：在输出的最终结果，显示物理内存与 swap 的总量 -s：可以让系统不断刷新显示数据 -c：与-s 同时处理，指定让 free 列出几次 jss@ubuntu:~$ free -m total used free shared buff/cache available Mem: 1970 485 1051 1 433 1335 Swap: 947 0 947 Mem 那一行显示的是物理内存的量，而 Swap 那一行显示的是内存交换分区的量。 total 是总量，used 是被已被使用的量，free 是剩余可用的量。后面的 shared、buffers、cached 则是在已被使用的量中，用来作为缓冲及缓存的。在执行读、写、执行文件时，这些文件会被系统暂时缓存下来，等待下次运行的时候可以更快速地取出，目的是为了提升系统地读写性能。这些 shraed、buffers、cached 的使用量中，在系统较为忙碌时，可以被发布而继续使用，因此后面有一个 available 数值。 Linux 系统为了提升系统性能，会将最常使用的或是最近使用的文件数据缓存下来，这样未来系统要使用该文件时，就可以直接由内存中查找取出，而不需要重新读取硬盘，速度上面当然加快了。因此，物理内存被用光是正常的。 而需要注意的是 swap 的量。一般来说，swap 最好不要被使用，尤其 swap 最好不要被使用超过 20%以上。因为 swap 的性能跟物理内存实在差太多，而系统使用到 swap 说明物理内存不足。 uname：查看系统与内核相关信息 uname [-asrmpi] 选项与参数： -a：所有系统相关的信息，包括下面的数据都会被列出来 -s：系统内核名称 -r：内核的版本 -m：本系统的硬件架构，例如 i686 或 x86-64 等 -p：CPU 的类型，与 -m 类似，只是显示的是 CPU 的类型 -i：硬件的平台 jss@ubuntu:~$ uname -a Linux ubuntu 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux 以举例来说，我的 Linux 主机使用的内核名称为 Linux，而主机名为 ubuntu，内核版本为 4.15.0-112-generic，该内核版本建立的日期为 2020-7-9，适用于 x86-64 及以上等级的硬件架构平台。 uptime：查看系统启动时间与任务负载 jss@ubuntu:~$ uptime 11:30:30 up 32 min, 1 user, load average: 0.00, 0.00, 0.00 这个命令很单纯，就是显示出目前系统已经运行的时间，以及 1、5、15 分钟内的平均负载情况。（其实就是显示 top 命令的最上面一行） netstat：追踪网络或 socket 文件 netstat 命令经常被用在网络监控方面。不过在进程管理方面也由很大的作用。基本设，netstat 的输出分为两大部分，分别是与网络较相关的部分和与系统进程叫相关的部分。 netstat -[atunlp] 选项与参数： -a：将目前系统上所有的连接、监听、socket 信息都列出来 -t：列出 tcp 网络封包的信息 -u：列出 udp 网络封包的信息 -n：不以进程的服务名称，以端口号（port number）来显示 -l：列出目前正在网络监听（listen）的服务 -p：列出该网络服务进程的 PID jss@ubuntu:~$ netstat Active Internet connections (w/o servers) // 与网络较相关的部分 Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 64 ubuntu:ssh 192.168.1.1:62401 ESTABLISHED Active UNIX domain sockets (w/o servers) // 与系统进程较相关的部分 Proto RefCnt Flags Type State I-Node Path unix 2 [ ] DGRAM 34711 /run/user/1000/systemd/notify unix 2 [ ] DGRAM 28069 /run/user/120/systemd/notify unix 2 [ ] DGRAM 19912 /run/systemd/journal/syslog unix 18 [ ] DGRAM 19914 /run/systemd/journal/dev-log unix 9 [ ] DGRAM 19924 /run/systemd/journal/socket ...（中间省略） unix 3 [ ] STREAM CONNECTED 28245 /run/user/120/bus 上面的结果显示了两个部分，分别是网络连接以及 Linux 上面的 socket 进程相关性部分。 首先来看看网络连接情况的部分： Proto：网络的封包协议，主要分为 TCP 和 UDP Recv-Q：非由用户进程连接到此 socket 的复制的总 Byte 数 Send-Q：非由远程主机传送过来的 acknowledged 总 Byte 数 Local Address：本地段的 IP:port 情况 Foreign Address：远程主机的 IP:port 情况 State：连接状态，主要有建立（ESTABLISED）以及监听（LISTEN） 网络连接部分仅有一条数据，它的意思是：通过 TCP 封包的连接，远程的 192.168.1.1:62401 连接到了本地端的 ubuntu:ssh，这条连接状态是建立（ESTABLISHED）的状态。 除了网络的连接之外，Linux 上的进程还可以接收不同进程所发过来的信息，那就是 Linux 上面的 socket 文件（socket file）。 上面的结果中 socket 文件的输出字段有： Proto：一般就是 unix RefCnt：连接到此 socket 的进程数量 Flags：连接的标识 Type：socket 存取的类型。主要有确认连接的 STREAM 与不需要确认的 DGRAM 两种 State：若为 CONNECTED 则表示多个进程之间已经建立连接 Path：连接到此 socket 的相关进程的路径，或是相关数据输出的路径 // 找出目前系统上已经在监听的网络连接及其进程 root@ubuntu:/home/jss# netstat -tulnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 438/systemd-resolve tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 685/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 512/cupsd tcp6 0 0 :::22 :::* LISTEN 685/sshd tcp6 0 0 ::1:631 :::* LISTEN 512/cupsd udp 0 0 127.0.0.53:53 0.0.0.0:* 438/systemd-resolve udp 0 0 0.0.0.0:68 0.0.0.0:* 698/dhclient udp 0 0 0.0.0.0:34978 0.0.0.0:* 507/avahi-daemon: r udp 0 0 0.0.0.0:5353 0.0.0.0:* 507/avahi-daemon: r udp 0 0 0.0.0.0:631 0.0.0.0:* 596/cups-browsed udp6 0 0 :::5353 :::* 507/avahi-daemon: r udp6 0 0 :::58335 :::* 507/avahi-daemon: r 使用此命令可以快速发现当前系统上哪些进程启动了哪些网络端口，对于不需要的网络服务，就可以手动执行kill -9命令结束进程。 dmesg：分析内核产生的信息 无论是启动的时候还是系统运行的过程中，只要是内核产生的信息，都会被记录到内存的某个包括区域中。dmesg 这个命令就能够将该区域的信息都出来。因为信息实在太多了，所以执行时可以加| more来使界面暂停。也可以配合grep命令快速找到想要的信息。 root@ubuntu:/home/jss# dmesg | grep -i disk [ 0.000000] RAMDISK: [mem 0x312c9000-0x3495bfff] [ 0.228831] VFS: Disk quotas dquot_6.6.0 [ 1.933245] sd 2:0:0:0: [sda] Attached SCSI disk vmstat：监控系统资源变化 vmstat [-afsSdp] [延迟 [总计检测次数]] 选项与参数： -a：使用 inactive/active（互动与否）替换 buffer/cache 的内存输出信息 -f：开机到目前未知，系统复制（fork）的进程数 -s：将一些事件（启动到目前未知）导致的内存变化情况列表说明 -S：后面可以接单位，指定显示的数据的单位 -d：列出磁盘的读写总量统计表 -p：后面列出分区，可显示该分区的读写总量统计表 // 统计目前主机 CPU 状态，每秒一次，共统计三次 root@ubuntu:/home/jss# vmstat 1 3 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 1074284 45104 401432 0 0 83 5 27 57 0 0 100 0 0 0 0 0 1074276 45104 401432 0 0 0 0 31 44 0 0 100 0 0 0 0 0 1074276 45104 401432 0 0 0 0 29 45 0 0 100 0 0 参数说明： 进程字段（procs） r：等待运行中的进程数量 b：不可被唤醒的进程数量 这两个项目越多，代表系统越忙碌。 内存字段（memery） swpd：虚拟内存被使用的容量 free：未被使用的内存容量 buff：用于缓冲存储器 cache：用于高速缓存 这部分与 free 命令相同。 内存交换分区（swap） si：由磁盘中将进程取出的容量 so：由于内存不足而将没用到的进程写入到磁盘的 swap 的容量 如果 si/so 的值太大，表示内存中的数据经常在磁盘与内存之间传输，系统性能会很差。 磁盘读写（I/O） bi：由磁盘读入的区块数量 bo：吸入到磁盘中的区块数量 这部分值越高，代表系统的 I/O 越忙碌。 系统（system） in：每秒被中断的进程次数 cs：每秒切换执行的事件切换次数 这两个数值越大，代表系统与外界设备的沟通越频繁。包括磁盘、网卡等。 CPU us：非内核层的 CPU 使用状态 sy：内核层的 CPU 使用状态 id：闲置的状态 wa：等待 I/O 所耗费的 CPU 状态 st：被虚拟机所使用的 CPU 状态 // 系统上面所有磁盘的读写状态 root@ubuntu:/home/jss# vmstat -d disk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur sec loop0 53 0 2096 136 0 0 0 0 0 0 loop1 49 0 2096 36 0 0 0 0 0 0 loop2 72 0 282 96 0 0 0 0 0 0 loop3 55 0 2128 136 0 0 0 0 0 0 loop4 60 0 704 92 0 0 0 0 0 0 loop5 73 0 290 80 0 0 0 0 0 0 loop6 41 0 672 68 0 0 0 0 0 0 loop7 9811 0 21550 5928 0 0 0 0 0 0 sda 13027 670 699778 12644 4844 881 63008 52376 0 5 sr0 0 0 0 0 0 0 0 0 0 0 loop8 1 0 2 0 0 0 0 0 0 0 详细字段含义可通过man vmstat命令查看，均与系统读写有关： FIELD DESCRIPTION FOR DISK MODE Reads total: Total reads completed successfully merged: grouped reads (resulting in one I/O) sectors: Sectors read successfully ms: milliseconds spent reading Writes total: Total writes completed successfully merged: grouped writes (resulting in one I/O) sectors: Sectors written successfully ms: milliseconds spent writing IO cur: I/O in progress s: seconds spent for I/O 特殊文件与进程 /proc/* 代表的意义 之前提到的所谓的进程都使在内存当中，而内存当中的数据都是写入到 /proc/* 这个目录下的。所以可以直接查看 /proc 这个目录当中的文件： oot@ubuntu:/home/jss# ls -l /proc total 0 dr-xr-xr-x 9 root root 0 8 月 23 10:57 1 dr-xr-xr-x 9 root root 0 8 月 23 10:57 10 ...（中间省略） -r--r--r-- 1 root root 0 8 月 23 13:59 vmstat -r--r--r-- 1 root root 0 8 月 23 13:59 zoneinfo 基本上，目前系统上面各个进程的 PID 都以目录的形式存在于 /proc 当中。比如我们启动所执行的第一个程序 systemd 的 PID 是 1，这个 PID 的所有相关信息都写入 /proc/1/* 当中： root@ubuntu:/home/jss# ls -l /proc/1 total 0 dr-xr-xr-x 2 root root 0 8 月 23 10:57 attr -rw-r--r-- 1 root root 0 8 月 23 14:01 autogroup -r-------- 1 root root 0 8 月 23 14:01 auxv -r--r--r-- 1 root root 0 8 月 23 10:57 cgroup --w------- 1 root root 0 8 月 23 14:01 clear_refs -r--r--r-- 1 root root 0 8 月 23 10:57 cmdline -rw-r--r-- 1 root root 0 8 月 23 10:57 comm -rw-r--r-- 1 root root 0 8 月 23 14:01 coredump_filter -r--r--r-- 1 root root 0 8 月 23 14:01 cpuset lrwxrwxrwx 1 root root 0 8 月 23 14:01 cwd -&gt; / -r-------- 1 root root 0 8 月 23 10:57 environ lrwxrwxrwx 1 root root 0 8 月 23 10:57 exe -&gt; /lib/systemd/systemd ...（以下省略） 里面的数据有很多，比较有趣的文件是： cmdline：这个进程被启动的命令串 environ：这个进程的环境变量内容 status：该进程的详细信息以及运行状态，包括内存占用状态等 /proc/ 目录下以数字命名的文件夹都是与特定 PID 相关的内容，如果是针对整个 Linux 相关的参数呢？那就是在 /proc/ 目录下的其他文件，其内容大致如下： 文件名 文件内容 /proc/cmdline 加载内核时所执行的相关命令和参数，查看此文件可了解命令时如何启动的 /proc/cpuinfo 本机的 CPU 相关信息，包含频率、类型与功能等 /proc/devices 这个文件记录了系统各个是主要设备的主要设备代号，与 mknod 有关 /proc/filesystems 目前系统已经加载的文件系统 /proc/interrupts 目前系统上面的 IRQ 分配状态 /proc/ioports 目前系统上面各个设备所配置的 I/O 地址 /proc/kcore 它的大小就是内存的大小（不要去读它） /proc/loadavg 还记得 top 以及 uptime？上面的三个平均数就是记录在这里的 /proc/meminfo 使用 free 列出的内存信息，实际上记录在这里 /proc/modules 已经加载的模块列表，也可以想成是驱动程序 /proc/mounts 系统已经挂载的数据 /proc/swaps 系统挂载的内存使用的硬盘分区 /proc/partitions 使用 fdisk -l 列出的硬盘分区，实际上记录在这里 /proc/uptime 使用 uptime 出现的信息，实际上记录在这里 /proc/version 内核的版本，就是用 uname -a 显示的内容 /proc/bus/* 一些总线的设备，还有 USB 的设备记录在此 可以使用 cat 命令查看上面这些文件，看过文件内容后，就会有更深的理解。 进程使用的文件 &amp; 使用文件的进程 fuser：通过文件找出正在使用该文件的进程 fuser [-umv] [-k [i] [signal]] file/dir 选项与参数： -u：除了进程的 PID 外，同时列出该进程的拥有者 -m：后面接的文件名会主动提到该文件系统的最顶层，对 umount 不成功很有效 -v：可以列出每个文件与进程还有命令的完整相关性 -k：找出使用该文件/目录的 PID，并试图以 SIGKILL 这个信号给予该 PID -i：必须与-k 配合，在执行前进行二次确认 -signal：例如-1、-15 等，若不加则是默认 SIGKILL(-9) root@ubuntu:~# fuser -uv . USER PID ACCESS COMMAND /root: root 1571 ..c.. (root)bash 通过 fuser 命令，可以找出使用指定文件、目录的进程，让我们了解到某个文件/文件夹目前正在被哪些进程所使用。 lsof：列出被进程所使用的文件名称 lsof [-aUu] [+d] 选项与参数： -a：多项数据需要同时成立才显示出结果时 -U：仅列出 UNIX-like 系统的 socket 文件类型 -u：后面接 username，列出使用者相关进程所使用的文件 +d：后面接目录，找出某个目录下面已经被使用的文件 直接使用该命令，将得到一大堆结果，因为该命令显示了所有进程及其所使用的文件。 配合各项参数以及 grep 命令使用，可找出某个进程使用的文件： // 列出属于 root 的 bash 这个程序所使用的文件 root@ubuntu:~# lsof -u root | grep bash bash 1571 root cwd DIR 8,1 4096 131074 /root bash 1571 root rtd DIR 8,1 4096 2 / bash 1571 root txt REG 8,1 1113504 262481 /bin/bash bash 1571 root mem REG 8,1 47568 798227 /lib/x86_64-linux-gnu/libnss_files-2.27.so bash 1571 root mem REG 8,1 97176 798167 /lib/x86_64-linux-gnu/libnsl-2.27.so bash 1571 root mem REG 8,1 47576 798289 /lib/x86_64-linux-gnu/libnss_nis-2.27.so bash 1571 root mem REG 8,1 39744 798224 /lib/x86_64-linux-gnu/libnss_compat-2.27.so bash 1571 root mem REG 8,1 4808688 397496 /usr/lib/locale/locale-archive bash 1571 root mem REG 8,1 2030544 798087 /lib/x86_64-linux-gnu/libc-2.27.so bash 1571 root mem REG 8,1 14560 798099 /lib/x86_64-linux-gnu/libdl-2.27.so bash 1571 root mem REG 8,1 170784 800215 /lib/x86_64-linux-gnu/libtinfo.so.5.9 bash 1571 root mem REG 8,1 170960 793808 /lib/x86_64-linux-gnu/ld-2.27.so bash 1571 root mem REG 8,1 26376 538353 /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache bash 1571 root 0u CHR 136,0 0t0 3 /dev/pts/0 bash 1571 root 1u CHR 136,0 0t0 3 /dev/pts/0 bash 1571 root 2u CHR 136,0 0t0 3 /dev/pts/0 bash 1571 root 255u CHR 136,0 0t0 3 /dev/pts/0 pidof：找出某个正在执行的进程的 PID pidof [-sx] program name 选项与参数： -s：仅列出一个 PID -x：同时列出该 program name 可能的 PPID 参考 《鸟哥的 LINUX 私房菜基础学习篇（第四版）》 ","link":"https://hill-jiang.github.io/post/Linux进程相关命令简介/"},{"title":"Streamlit：不写前端开发WEB应用","content":"背景 最近组内用 Flask+前端 开发了个基于 web 的图片评分筛选可视化工具，用于 AI NVR 项目的人脸图片分类。 虽然我没有参与 AI NVR 的项目，但是对此类 web 应用比较感兴趣，本身最近也想学一下 Python 后端开发，又由于初版的工具功能还不完善，需要添加很多功能，所以想着优化一下。 本来打算是用最近比较火的 FastAPI 写的，跟 Flask 差不多，很多地方可以复用，但是上手之后开始写前端的时候，就在 html 和 js 上遇到了层层阻碍。毕竟没有系统地学过前端，每遇到一个问题都要网上查半天，效率实在低。 突然想起来以前折腾可视化时候遇到的一个工具：Stremlit，于是重新翻出来研究了一下，发现还是比较好用的，写篇博客记录一下。 Streamlit 简介 Stremlit 是一个开源的 Python 库，提供简便的 API 接口，可以快速构建界面美观、交互友好的 web 应用程序，比较常用于结果演示、信息数据可视化等领域。 安装 依赖 Python 3.6 - Python 3.8 pip intsall stremlit 运行 不同于一般 Python 脚本的运行，streamlit 运行需要使用以下命令： stremlit run [filename].py 默认使用 8501 端口，运行后会自动打开浏览器访问http://localhost:8501 基本使用 前面提到 streamlit 的一大特点就是方便快捷（不用写前端），下面就介绍一下怎么用几行 Python 代码就在网页上添加各种元素。 各个函数对应的功能不多赘述，直接看效果即可。 文本显示 import streamlit as st st.text('Fixed width text') st.markdown('_Markdown_') # see * st.latex(r''' e^{i\\pi} + 1 = 0 ''') st.write('Most objects') # df, err, func, keras! st.write(['st', 'is &lt;', 3]) # see * st.title('My title') st.header('My header') st.subheader('My sub') st.code('for i in range(8): foo()') 数据显示 import streamlit as st import pandas as pd my_dataframe = pd.DataFrame({ &quot;1&quot;: {&quot;A&quot;: 1, &quot;B&quot;: 2}, &quot;2&quot;: {&quot;A&quot;: 11, &quot;B&quot;: 22}, &quot;3&quot;: {&quot;A&quot;: 'a', &quot;B&quot;: 'b'} }) st.dataframe(my_dataframe) st.table(my_dataframe.iloc[0:1]) st.json({'foo': 'bar', 'fu': 'ba'}) 交互控件 import streamlit as st st.button('Hit me') st.checkbox('Check me out') st.radio('Radio', [1,2,3]) st.selectbox('Select', [1,2,3]) st.multiselect('Multiselect', [1,2,3]) st.slider('Slide me', min_value=0, max_value=10) st.select_slider('Slide to select', options=[1,'2']) st.text_input('Enter some text') st.number_input('Enter a number') st.text_area('Area for textual entry') st.date_input('Date input') st.time_input('Time entry') st.file_uploader('File uploader') st.color_picker('Pick a color') 状态和进度 import streamlit as st import time st.progress(50) st.spinner() with st.spinner(text='In progress'): time.sleep(5) st.success('Done') st.balloons() st.error('Error message') st.warning('Warning message') st.info('Info message') st.success('Success message') st.exception('Error') 图表 streamlit 本身支持的图表类型较少，仅有线形图、面积图、柱状图，且不支持坐标轴、标题自定义等功能。 但是，stremlit 支持嵌入很多功能丰富的第三方图表，包括 matplotlib、altair、seaborn 等，能够满足各类图表的绘制需求。 st.line_chart(data) st.area_chart(data) st.bar_chart(data) st.pyplot(fig) st.altair_chart(data) st.vega_lite_chart(data) st.plotly_chart(data) st.bokeh_chart(data) st.pydeck_chart(data) st.deck_gl_chart(data) st.graphviz_chart(data) st.map(data) 仅展示了streamlit自带的基本图表 视频/音频/图片 st.image('test.png') st.audio('test.mp3') st.video('test.mp4') 布局管理 import streamlit as st with st.beta_container(): st.write('123') cols = st.beta_columns(3) with cols[0]: st.write('First col') with cols[1]: st.write('Second col') with cols[2]: st.write('Third col') with st.beta_expander(&quot;展开&quot;): st.write(&quot;展开的信息&quot;) 侧边栏 st.sidebar()将定义一个侧边栏，可以在侧边栏中添加各类控件 其他 streamlit 还支持页面主题自定义、应用发布、页面录制等功能，可自行探索使用。 贴一下 cheatsheet（这个 cheatsheet 也是用 streamlit 实现的） 实际应用 花了一段时间，写了一个图片分类的工具 左侧使用侧边栏，slider控件用于设置各类筛选条件的范围，radio控件用于设置排序条件，button控件用于导出筛选后的图片，number_input控件用于对图片进行主观评分。 中间显示当前的筛选条件，排序条件，筛选的图片结果，图片按照指定的列宽度布局，下方显示该图片的信息。 最后还对各类图片参数的分布进行了统计，绘制了直方图。 后台的筛选排序逻辑用 Pandas.dataFrame 实现，不多介绍。 虽然页面看上去还行，功能也基本可用，但性能实在是有点拉胯，2000张图片基本上要等待 10 秒左右才能完全加载出来，且由于每次修改筛选或排序条件，页面都要重新进行加载，所以使用起来体验还是很差，对于该需求来说，实用性不强。 性能瓶颈应该还是在大量图片的加载和显示，实测如果不显示图片能够做到页面秒响应。 总结 Streamlit 作为一个 Python Web 开发工具，优缺点都很明显 优点： 上手简单，开发容易，不用写前端 界面美观，功能较丰富 缺点： 相比一般的后端开发框架，性能较差 由于不能自己写前端，有一定的局限性 这次用 Streamlit 开发的图片分类工具使用体验不太能够接受，可能还是要考虑用其他工具开发，但 Steamlit 本身算是非常好用的工具了。 后续工作中如果有其他需求，适合用 Streamlit 进行开发的，再进行考虑吧。 参考 https://github.com/stremlit https://docs.streamlit.io https://streamlit.io/gallery ","link":"https://hill-jiang.github.io/post/Streamlit：不写前端开发WEB应用/"},{"title":"使用 Python 操作 SQLite","content":"关于SQLite SQLite是一款轻量数据库，占用资源少，普遍应用于嵌入式系统中。 Python的SQLite操作 SQLite3可使用sqlite3模块与Python进行集成。Python 2.5以上的版本默认安装了该模块，因此不用额外进行安装。 常用API sqlite3.connect(database [, timeout, other optional arguments]) 该API打开一个到SQLite数据库文件database的链接。如果数据库打开成功，则返回一个连接对象。 如果给定的数据库名称不存在，则该调用将创建一个数据库。 connection.cursor([cursorClass]) 该API创建一个cursor，将在Python数据库编程中用到。 cursor.execute(sql [, optional parameters]) 执行一个SQL语句。该SQL语句可以被参数化（即使用占位符替代SQL文本）。 例如：cursor.execute(&quot;insert into people values (?, ?)&quot;, (who, age)) cursor.fetchone() 该方法获取查询结果集中的下一行，返回一个单一的序列。当没有更多可用的数据时，则返回None。 cursor.fetchall() 该方法获取查询结果集中所有的行，返回一个列表。当没有可用的行时，返回一个空列表。 connection.commit() 提交当前的事务。如果未调用该方法，那么自上一次调用commit()以来所做的任何动作对其他数据库连接来说是不可见的。 connection.rollback() 回滚自上一次调用commit()以来对数据库所做的更改。 connection.close() 关闭数据库连接。注意，这不会调用commit()，如果未调用任何commit()就关闭数据库连接，所做的更改将全部丢失。 举例 连接数据库 import sqlite3 conn = sqlite3.connect('test.db') 执行上述程序，将会在当前目录创建数据库test.db。 创建表 import sqlite3 conn = sqlite3.connect('test.db') c = conn.cursor() c.execute('''CREATE TABLE COMPANY ( ID INT PARIMARY KEY NOTE NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL);''') conn.commit() conn.close() 执行上述程序，将会在test.db中创建COMPANY表。 INSERT操作 import sqlite3 conn = sqlite3.connect('test.db') c = conn.cursor() c.execute(&quot;INSERT INTO COMPANY (ID, NAME, AGE, ADDRESS, SALARY) VALUES (1, 'Paul', 32, 'California', 20000)&quot;) conn.commit() conn.close() 执行上述程序，将会在COMPANY表中新增一个条目。 SELECT操作 import sqlite3 conn = sqlite3.connect('test.db') c = conn.cursor() cursor = c.execute(&quot;SELECT * FROM COMPANY&quot;) for row in cursor: print(row) conn.close() 执行上述程序，将会输出以下结果： (1, 'Paul', 32, 'California', 20000.0) UPDATE操作 import sqlite3 conn = sqlite3.connect('test.db') c = conn.cursor() c.execute(&quot;UPDATE COMPANY set SALARY = 2500 where ID=1&quot;) conn.commit() conn.close() 执行上述程序，将会把ID为1的条目的SALARY更新为2500。 DELETE操作 import sqlite3 conn = sqlite3.connect('test.db') c = conn.cursor() c.execute(&quot;DELETE from COMPANY where ID=1&quot;) conn.commit() conn.close() 执行上述程序，将会把ID为1的条目删除 参考 菜鸟教程-SQLite教程-Python ","link":"https://hill-jiang.github.io/post/使用Python操作SQLite/"},{"title":"基于 PyQt 的桌面小程序开发","content":"PyQt简介 PyQt 是创建 GUI 应用程序的一个工具包，融合了 Python 和 Qt 库，是 Python 中 GUI 设计最强大的库之一，具有界面美观、跨平台、文档教程多等优点。 PyQt 有功能丰富函数和方法，基本上能够实现绝大部分设计要求；但正是因为此，其功能复杂，接口繁多，学习成本较高。 PyQt 的主要模块有： QtGui：涵盖了多种基本图形的功能的类 QtWidgets：包含整套的 UI 元素控件 QtCore：涵盖了核心的非 GUI 功能，可用来处理时间、文件、目录、数据类型、文本流、链接等对象 刚开始上手 PyQt 的时候，画控件成为了很令人头疼的问题，使用代码进行控件布局绘制和调整非常不直观，比较费时间。后来发现了简单的控件绘制工具——Qt Designer。Qt Designer 是交互式可视化 GUI 设计工具，可以帮助我们快速绘制 UI 界面。 下面就以我开发的一个用来测试 TDCP 和 TUMS 接口的小程序为例，介绍 PyQt 的开发流程。 Qt Designer 安装 安装 PyQt5 和 QtDesigner pip install PyQt5 pip install PyQt5-tools 安装完后，Qt Designer 的路径为：Python安装路径\\Lib\\site-packages\\qt5_applications\\Qt\\bin\\designer.exe UI 设计 启动后，首先新建一个 Widget 然后将左侧 Widget Box 中的控件按照个人设计拖拽到打开的窗口中，自行修改大小和布局，双击修改名称，并且可以在右侧的属性编辑器中根据需要修改属性（仿佛回到了高中信息技术课上学 VB）。 设计过程中，使用快捷键 Ctrl-R 可以快速预览。 我设计完的页面如下： 转化为 py 文件 完成设计后，保存输出的文件是 .ui 格式的，可以使用 PyUIC 工具转为 .py 文件： pyuic5 -o 目标文件名.py 源文件名.ui 程序设计 基本框架 上述 ui 文件转化完的 py 文件中，仅有一个Ui_Form的类，包含setupUi和retranslateUi两个方法，没有程序入口，并不能直接运行。 按照视图与逻辑分离的原则，我们再编写一个新的类来调用这个方法。 import sys from PyQt5.QtWidgets import QApplication, QMainWindow from PyQt5 import QtCore, QtWidgets class Ui_Form(object): ... class MyWindow(QMainWindow, Ui_Form): def __init__(self): QMainWindow.__init__(self) self.setupUi(self) if __name__ == '__main__': app = QApplication(sys.argv) window = MyWindow() window.show() sys.exit(app.exec_()) 此时，运行该脚本就会出现我们绘制的 UI 窗口了： UI调整 由于界面上显示的各类元素本质上都在 python 代码中定义和实现，所以我们可以在代码中对其进行调整，比如修改程序的标题，只需要在retranslateUi方法中修改一行代码： Form.setWindowTitle(_translate(&quot;Form&quot;, &quot;简易发包工具For TUMS/TDCP&quot;)) 控件逻辑添加 上述我们完成的部分只是一个 UI 界面，并没有任何功能，因此，我们还需要在程序中为各类控件添加逻辑，才能实现我们想要的功能。 下面列举一些常用的方法： 将点击按钮绑定到某一方法 pushButton.clicked.connect(method_name) def method_name(): ... 获取输入框内容 line = lineEdit.text() text = textEdit.toPlainText() 设置文本输入框内容 textEdit.setText(text) 至于如何将各类控件逻辑组合起来，实现完整的功能，属于 python 程序设计的部分，在此不做说明。 打包与发布 使用 pyinstaller 进行打包，可以把源文件打包成 exe 文件，这样使用时不用依赖 python 环境，可以直接拷贝 exe 文件到其他 Windows PC 上使用。 打包方法 先安装 pyinstaller： pip install pywin32 pip install pyinstaller 然后使用 pyinstaller 打包 pyinstaller -F -w postTool.py 完成后，会在当前目录下生成一个 spec 文件和build、dist两个文件夹，可执行的 exe 文件就在dist文件夹中，双击即可运行。 在此提供我写的小工具主程序部分代码和可执行 exe 文件： 发送 TDCP/TUMS 请求部分由于涉及到相关登录校验规则，在此不提供。 缺陷 使用 pyinstaller 打包会把各种依赖库都一起打包进去，导致生成的 exe 文件很大，且启动速度很慢。我写完的程序打包好占用接近 35 MB，启动大约需要 5 秒。 尝试过各种优化方式，比如建立 python 虚拟环境减少打包依赖、换用其他打包方式等，效果都不明显。而在测试部广泛使用的 Windows 小程序postData.exe（基于 MFC 开发），同样用来发送 TDCP 请求，大小只有 100 K，且启动很快。 MFC 是微软提供的一个 C++ 类库 由此体会到 Python 的局限性还是很大啊。 后续 最近因为装的 Python 库越来越多，导致新开发的 PyQt GUI 工具打包越来越大，所以还是用虚拟环境打包吧，补充一下虚拟环境打包方法： 安装 virtualenvs pip install virtualenv 进入项目根路径，创建虚拟环境 virtualenv venv 激活虚拟环境 cd venv/Scripts activate 执行完后，可以看到命令行名称前出现了(venv)，说明虚拟环境激活成功 4. 安装项目依赖库 因为新激活的虚拟环境不包含任何依赖库，所以需要先手动将该工具项目所需要的依赖安装一下，直接用pip安装即可，装完之后手动执行一下python xxx.py看看能不能运行 5. 打包 回到项目根路径 pyinstaller -p venv/Scripts -F -w xxx.py ","link":"https://hill-jiang.github.io/post/基于PyQt的桌面小程序开发/"},{"title":"安卓 APP 抓包探索","content":"背景 最近有个项目模块，支持 100 路并发的视频分享预览，需进行测试覆盖。 找 100 个设备同时登录不太可能，即使在 PC 上用模拟器，按照之前的经验，性能比较好的 PC 一台也就能跑 10 个左右，凑不出那么多空闲 PC 用于测试。于是想到用脚本模拟 APP 从登录到预览的整个流程，只拉流不解码，对性能要求比较低，理论上一台 PC 就能实现。 申了一圈文档权限，发现从文档出发整理整个流程还是比较困难，所以决定先用实际 APP 抓包，再结合文档整理接口流程。 代理 说到抓包可能最先想到的就是 Wireshark，诚然用 Wireshark 配置无线抓包、或是在设备上直接跑 tcpdump 再导出成抓包文件用 Wireshark 打开，是可以抓到所有 APP 发送和接受的请求的。但问题在于，这部分请求是 HTTPS 加密的，我们无法直接看到交互内容，此方法也就行不通。 HTTPS 的数据包虽然能抓到，但无法查看内容 移动端抓包工具基本采用的都是中间人攻击的方式（MITM），简单来说就是在服务器和客户端中间做代理，截获两边发往对端的数据。 比较有名的抓包工具，例如 Fiddler、Charles 等，可以对 APP 进行抓包，其原理是抓包功能工具开一个 HTTP 代理，将手机与 PC 连入同一网络，配置手机的代理为工具的代理端口，就可以实现中间人攻击抓包。 但是，我司的物联 APP 并不使用安卓系统的 HTTP 代理，而是在 APP 内部自建 Client 直接与服务器建立 SSL 连接进行交互，所以此时即使配置了代理，流量也不会走到代理工具上去，也就抓不到包。 VPN 直接的 HTTP 代理行不通，还有 VPN 的方式，通过该方式在客户端与抓包工具之间的 VPN 连接，实现所有流量经过抓包工具，从而进行拦截。 安卓端可以直接用 HttpCanary 来自建 VPN 服务器进行抓包；PC 端也有类似的工具，推荐 HTTP Toolkit，界面友好功能强大。 certificate pinning 无论是代理还是 VPN，想对 HTTPS 的加密数据进行解析，都必须先在客户端系统上安装由抓包工具提供的虚假 CA 证书。 客户端在发送 HTTPS 请求时，系统会先对服务器发来的证书进行校验，由于已经在系统上安装了抓包工具提供的证书，所以校验自然能通过。 然而我们的物联 APP，并不适用系统的证书对服务器发来的证书进行校验，而是用了称为 certificate pinning 的方法，使用内置的自签证书来对服务器证书进行校验，此时校验失败，客户端并不信任抓包工具发来的证书。结果就是客户端根本不会把数据发给我们的抓包工具，在 APP 上表现就是直接提示“网络错误”。 可以看到手机的其他数据包都能抓到，但 TP-LINK 物联 APP 相关的请求未发送出去。 Frida 那么有没有解决方案呢，肯定是有的，可以使⽤例如 xposed 框架 + justtrustme 模块或 VirtualApp 等⼯具对证书验证的函数进⾏ hook，使其在证书校验时始终返回成功，即可绕过证书验证。但是按照其提供的参考链接中的方法，没能直接 hook 到校验证书的 API。 在 HTTP Toolkit 的官网，推荐是使用 Frida 工具绕过 certificate pinning 的验证，原理基本相同。此部分还需要对手机进行 root，使用 adb 连接手机（root 有风险，操作需谨慎，我是找了一个不用的旧手机）。 但是我按照步骤安装配置完，并行不通。 研发提到app是在native层使⽤openssl做的证书验证，所以justtrustme没有hook到，这时我大概明白了，证书验证的部分并没有写在 java 源码里，所以 hook 不出来，于是我就去搜索“APP native层openssl证书验证”，没想到居然找到一篇对我司安防 APP 进行抓包的博客：逆向破解 | 利用Frida手动绕过Android APP证书校验。 他同样是用 Frida 工具进行 hook，我直接 copy 了他的 js 脚本： Java.perform(function(){ var nativePointer = Module.findExportByName(&quot;libIPCAppContextJNI.so&quot;, &quot;X509_verify_cert&quot;); console.log(&quot;-------------Start-------------&quot;); Interceptor.attach(nativePointer, { onEnter: function(args){ // 此处是修改入参要做的逻辑，在这里不需要修改，留空即可 }, onLeave: function(retval){ console.log(&quot;----------leaving-------------&quot;) // 打印原始的返回值 console.log(retval); // replace修改返回值 retval.replace(0x1) // 再次打印出来验证一下修改是否成功 console.log(retval); } }); }); 由于有注释，理解起来也不难，就是在需要对证书进行校验时，让它直接返回校验成功。 跑了一遍，直接报错expect a pointer，在 Frida 中直接执行Module.findExportByName(&quot;libIPCAppContextJNI.so&quot;, &quot;X509_verify_cert&quot;)返回了null，感觉应该是 APP 版本更新修改了一些依赖库。 再找了一些其他人写的用 Frida 绕过 certificate pinning 验证的 js 脚本进行参考，把上面的脚本修改成： Java.perform(function() { const System = Java.use(&quot;java.lang.System&quot;); const Runtime = Java.use(&quot;java.lang.Runtime&quot;); const SystemLoad_2 = System.loadLibrary.overload(&quot;java.lang.String&quot;); const VMStack = Java.use(&quot;dalvik.system.VMStack&quot;); SystemLoad_2.implementation = function(library) { console.log(&quot;Loading dynamic library =&gt; &quot; + library); try { const loaded = Runtime.getRuntime().loadLibrary0(VMStack.getCallingClassLoader(), library); // here your hook Interceptor.attach(Module.findExportByName(&quot;lib&quot;+library+&quot;.so&quot;, &quot;X509_verify_cert&quot;), { onEnter: function(args) { // 不做任何事 }, onLeave: function(retval){ // 如果校验失败 if(retval == 0x0) { console.log(library); // replace修改返回值 retval.replace(0x1); // 再次打印出来验证一下修改是否成功 console.log(retval); } } }); return loaded; } catch(ex) { console.log(ex); } }; }); 就是在所有库调用时，都判断证书校验部分的返回值，如果返回校验失败就手动改成校验成功，用此方法，终于是抓到了我们的 APP 发出去的包，然而： 看到 Status 鲜红的 502，我的心又凉了半截。再一看右边的说明，是服务器的 HTTPS 证书未被信任，这可太熟悉了，和我们的 VMS 系统首次访问浏览器提示”你的连接不是专用连接“一个道理，忽略就行了，只可惜这个工具让我掏钱”Get Pro“版本才能继续。 本来想着反正已经能抓到 APP 发出去的请求了，看不到响应也问题不大，但此时发现 APP 卡死了，过了一会直接闪退，想了一下应该是发出去的请求都没有收到响应，可能就卡在某一步了，依赖前面的响应的请求，后面也没法发，看来工作还得继续。 既然我已经用 Frida 在客户端上实现了绕过证书验证，那说明抓包工具应该已经能实现抓包了，只不过是 HTTP Toolkit 这个中间人不给我转发，我拿不到响应。于是又想到 HttpCanary。 在 HttpCanary 上一试，还是没抓到，但是有很多 Warnning，点开 Warning： 解决方案是用 Xposed 模块 JustTrustMe 解除证书固定，这部分我已经用 Frida 做了，那么我导入该自签证书应该就可以了吧： 终于，想要的结果出现了： 总结 APP 的 HTTPS 抓包常用 Fiddler、Charles 等工具，但我司物联 APP 不走系统代理，则抓不到包 用 HttpCanary、HTTP Toolkit 等可以自建 VPN 进行流量拦截抓包，但我司物联 APP 用了 certificate pinning，使用自签证书进行校验，导致校验失败抓不到包 使用 xposed + justtrustme 或 Frida 等工具对证书验证的函数进⾏hook，使其始终返回成功可绕过证书验证，但我司物联 APP 的证书验证并未写在 Java 层，导致 hook 不到 Frida 可直接对调用的库进行 hook，使用此方法可成功绕过证书验证 HTTP Toolkit 工具免费版对于证书未信任的服务器不会发送请求 HttpCanary 在绕过证书验证后可发送请求并接受响应，能正常抓包 至此，在折腾了一大番之后，完成了想要抓个包的小目标。 只能说学无止境，对不了解的领域还是要多探索积累。 参考 https://httptoolkit.tech/docs/guides/android/ https://httptoolkit.tech/blog/frida-certificate-pinning/ http://www.52bug.cn/cracktool/6380.html https://stackoverflow.com/questions/60794720/frida-hook-native-method-failure-on-android-q ","link":"https://hill-jiang.github.io/post/安卓 APP 抓包探索/"},{"title":"证件校验规则研究","content":"身份证 身份证格式 身份证为18位，构成规则如下： 前6位表示出生地所在县（市、区） 7~14位表示出生年月日 15~17位表示顺序码，对同地区、同年同月同日出生的人进行排序，其中第17位奇数分给男性，偶数分给女性 第18位为校验码，校验码如果出现数字10，用X来表示 校验码生成 将身份证号码的前17位分别乘以不同的系数。从第1位到第17位的系数分别为：7-9-10-5-8-4-2-1-6-3-7-9-10-5-8-4-2。 将这17位数字和系数相乘的结果相加。 将相加得到的和除以11，取余数，余数只可能为0~10，分别对应身份证最后一位：1-0-X-9-8-7-6-5-4-3-2。 Python检验校验码 factor = [7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2] verify_list = ['1', '0', 'X', '9', '8', '7', '6', '5', '4', '3', '2'] number = input() checksum = 0 for i in range(len(number) - 1): checksum = checksum + factor[i] * int(number[i]) checksum = checksum % 11 print(verify_list[checksum]) if number[-1] == verify_list[checksum]: print(&quot;Pass&quot;) else: print(&quot;Fail&quot;) 合法地区列表 11：北京，12：天津，13：河北，14：山西，15：内蒙古； 21：辽宁，22：吉林，23：黑龙江； 31：上海，32：江苏，33：浙江，34：安徽，35：福建，36：江西，37：山东； 41：河南，42：湖北，43：湖南，44：广东，45：广西，46：海南； 50：重庆，51：四川，52：贵州，53：云南，54：西藏； 61：陕西，62：甘肃，63：青海，64：宁夏，65：新疆； 71：台湾； 81：香港，82：澳门； 91：国外。 测试点 不仅需要测试是否对身份证号的这些字段有校验，还需要测试是否将合法的证件号识别为非法。 因为证件号的校验过程中，可能还会因校验代码某处出错，导致合法证件号不通过的情况，这些情况所需要构造的测试输入较多。 与将非法的证件号识别为合法相比，将合法的证件号识别为非法风险更大，所以需要进行测试覆盖。 地区 合法地区列表取边界值 通过：11、15、21、23、31、37、41、46、50、54、61、65、71、81、82、91 不通过：00、01、09、10、16、20、24、30、38、40、47、55、60、66、70、72、80、83、90、92 对第3-6位不做校验，采取随机数生成的方式 出生年份 1900-当前年份取边界值 通过：1900、1999、2000、2001、当前年份（小于等于当前日期） 不通过：1899、2021 出生月份 01-12取边界值 通过：01、09、10、11、12 不通过：00、13 出生日期 01-31取边界值 通过：01、10、11、20、21、30、31 不通过：00、32 年月日组合 闰平年2月日期校验、每月日期校验 通过：20040229、0131、0228、0331、0430、0531、0630、0731、0831、0930、1031、1130、1231、当前日期 不通过：20010229、0132、0230、0332、0431、0532、0631、0732、0832、0931、1032、1131、1232、当前日期+1天 顺序码 000-999取边界值 通过：000、099、100、101、999 校验码 遍历0-9、X 通过：遍历0-9、X，且按照校验码计算规则生成 不通过：不按照校验码计算规则生成 用例生成 首先，测试应该验证通过的组合 除校验码外的选项进行单一选择组合 为每一个生成的结果构造身份证号，结果如下： 110692190001010004 154028199909100999 213694200010111005 233467200111201011 313659190012219990 370592199901300000 412401190008310991 467055200402291001 506681190001311017 542106199902289993 619982200003310007 656983200104300992 717280190005311008 815268199906301016 82094220000731999X 912217200108310002 112780190009300999 157117199910311004 21574119001130101X 233436199912319992 还需要额外增加一个当前日期构造的身份证号 已覆盖所有校验码的情况 然后验证应该验证不通过的组合 对于每一个非法的字段，组合其他合法的字段，并生成正确的校验码 地区非法： 00821019960110000X 016104199601100994 090596199601101008 106766199601101013 161241199601109994 207667199601100006 248306199601100992 301342199601101003 380558199601101019 406436199601109999 479475199601100007 559877199601100999 604832199601101001 666767199601101019 708179199601109995 721966199601100002 800764199601100995 831137199601101005 908908199601101011 929776199601109990 出生年份非法： 119266189901100000 155809202101100992 出生月份非法： 216299199600101007 233888199613101010 出生日期非法： 312544199601009998 271079199601320009 还需要一个当前日期+1天构造的身份证号，可使用附件脚本构造 年月日组合非法： 418429200102290995 465225199601321002 503522199602301011 542888199603329990 619936199604310009 650394199605320993 710026199606311009 817812199607321011 825244199608329990 914952199609310004 116092199610320999 15948219961131100X 219376199612321010 校验码非法： 112409199001010005 ","link":"https://hill-jiang.github.io/post/证件校验规则研究/"},{"title":"Python 自动化测试工具 Playwright","content":"Github 地址：https://github.com/microsoft/playwright-python 简介 Playwright 是一个微软开源的浏览器自动化端到端测试工具，有了它就可以方便地对网页应用程序执行端到端测试。还支持 Linux、macOS、Windows 系统下的 Chromium、Firefox 和 WebKit 浏览器。 安装 python -m pip install playwright python -m playwright install 将同时安装 Playwright 以及 Chromium、Firefox 和 WebKit 的浏览器驱动依赖。 使用 codegen Playwright 支持记录用户在浏览器中的操作并且自动生成代码。 python -m playwright codegen Playwright 支持同步 API 以及异步 API，它们在功能方面是相同的，只是在使用API的方式上不同。 同步 API demo from playwright import sync_playwright with sync_playwright() as p: for browser_type in [p.chromium, p.firefox, p.webkit]: browser = browser_type.launch() page = browser.newPage() page.goto('http://whatsmyuseragent.org/') page.screenshot(path=f'example-{browser_type.name}.png') browser.close() 异步 API demo import asyncio from playwright import async_playwright async def main(): async with async_playwright() as p: for browser_type in [p.chromium, p.firefox, p.webkit]: browser = await browser_type.launch() page = await browser.newPage() await page.goto('http://whatsmyuseragent.org/') await page.screenshot(path=f'example-{browser_type.name}.png') await browser.close() asyncio.get_event_loop().run_until_complete(main()) 其他 Playwright 还支持 pytest 测试、交互模式运行、执行 JS 代码、记录网络请求等功能。 体验 安装 安装浏览器依赖的时候很慢，即使已经 FQ codegen 先来试试自动生成代码的功能 C:\\Users\\Hill\\Desktop&gt;python -m playwright codegen --help Usage: index codegen [options] [url] open page and generate code for user actions Options: -o, --output &lt;file name&gt; saves the generated script to a file --target &lt;language&gt; language to use, one of javascript, python, python-async, csharp (default: &quot;python&quot;) -h, --help display help for command Examples: $ codegen $ codegen --target=python $ -b webkit codegen https://example.com -o指定输出文件，--target指定输出语言，-b指定使用的浏览器类型。 就用使用百度搜索Python来试一下 python -m playwright codegen -o codegen_test.py https://www.baidu.com 执行完后，会自动打开一个浏览器，跳转到百度，输入 Python 后回车，然后再关闭浏览器，运行目录下就出现了codegen_test.py，文件内容如下： from playwright import sync_playwright def run(playwright): browser = playwright.chromium.launch(headless=False) context = browser.newContext() # Open new page page = context.newPage() # Go to https://www.baidu.com/ page.goto(&quot;https://www.baidu.com/&quot;) # Click input[name=&quot;wd&quot;] page.click(&quot;input[name=\\&quot;wd\\&quot;]&quot;) # Press CapsLock page.press(&quot;input[name=\\&quot;wd\\&quot;]&quot;, &quot;CapsLock&quot;) # Fill input[name=&quot;wd&quot;] page.fill(&quot;input[name=\\&quot;wd\\&quot;]&quot;, &quot;P&quot;) # Press CapsLock page.press(&quot;input[name=\\&quot;wd\\&quot;]&quot;, &quot;CapsLock&quot;) # Fill input[name=&quot;wd&quot;] page.fill(&quot;input[name=\\&quot;wd\\&quot;]&quot;, &quot;Python&quot;) # Press Enter page.press(&quot;input[name=\\&quot;wd\\&quot;]&quot;, &quot;Enter&quot;) # assert page.url() == &quot;https://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=1&amp;rsv_idx=1&amp;tn=baidu&amp;wd=Python&amp;fenlei=256&amp;rsv_pq=e0a2bd93007a18fd&amp;rsv_t=af15WBQFDyMcGk%2B3Xs%2BRwDVBT40f2Z8itvMfT5sBiqICLeozAk0f0ou7Pao&amp;rqlang=cn&amp;rsv_enter=1&amp;rsv_dl=tb&amp;rsv_sug3=7&amp;rsv_sug1=6&amp;rsv_sug7=100&amp;rsv_sug2=0&amp;rsv_btype=i&amp;prefixsug=Python&amp;rsp=5&amp;inputT=2425&amp;rsv_sug4=3717&quot; # Close page page.close() # --------------------- context.close() browser.close() with sync_playwright() as playwright: run(playwright) 重新执行该 Python 文件，将重复一遍刚刚的操作，自动启动浏览器，访问百度并搜索关键词Python，由于停留时间太短，所以要在page.close()前面加一个等待时间观察效果 同步和异步 API 测试 参照官方 demo 编写了同步和异步 API 测试脚本： import asyncio from playwright import sync_playwright, async_playwright def sync_demo(): with sync_playwright() as p: for browser_type in [p.chromium, p.firefox, p.webkit]: browser = browser_type.launch() page = browser.newPage() page.goto('http://www.baidu.com') page.screenshot(path=f'sync_example-{browser_type.name}.png') browser.close() async def async_demo(): async with async_playwright() as p: for browser_type in [p.chromium, p.firefox, p.webkit]: browser = await browser_type.launch() page = await browser.newPage() await page.goto('http://www.baidu.com') await page.screenshot(path=f'async_example-{browser_type.name}.png') await browser.close() sync_demo() asyncio.get_event_loop().run_until_complete(async_demo()) 运行时，依次打开三种不同的浏览器，访问百度首页并截图，并且由于默认使用的是 headless 模式，所以不会显示浏览器页面 headless 模式可以使用browser = browser_type.launch(headless=Flase)禁用 记录网页请求 from playwright import sync_playwright with sync_playwright() as p: browser = p.chromium.launch() page = browser.newPage() def log_and_continue_request(route, request): print(request.url) route.continue_() # Log and continue all network requests page.route('**', lambda route, request: log_and_continue_request(route, request)) page.goto('http://www.baidu.com') browser.close() 执行后，会打印访问百度首页时，浏览器发送的所有请求： http://www.baidu.com/ https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/baiduyun@2x-e0be79e69e.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/zhidao@2x-e9b427ecc4.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/baike@2x-1fe3db7fa6.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/tupian@2x-482fc011fc.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/baobaozhidao@2x-af409f9dbe.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/wenku@2x-f3aba893c1.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/jingyan@2x-e53eac48cb.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/topnav/yinyue@2x-c18adacacb.png https://www.baidu.com/img/gjrdong_5dbd765f1fd82a0771cbc88aec7341c8.gif https://www.baidu.com/img/flexible/logo/pc/result.png https://www.baidu.com/img/flexible/logo/pc/result@2.png https://www.baidu.com/img/flexible/logo/pc/peak-result.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/qrcode/qrcode@2x-daf987ad02.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/qrcode/qrcode-hover@2x-f9b106a848.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/lib/jquery-1-edb203c114.10.2.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/lib/esl-ef22c5ed31.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/sbase-0948aa26f1.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/s_super_index-855fcfd82e.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/min_super-ce30feac9c.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/components/hotsearch-8f112f3361.js https://hectorstatic.baidu.com/cd37ed75a9387c5b.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/bundles/polyfill_9354efa.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/global/js/all_async_search_dafef7e.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/plugins/every_cookie_4644b13.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/plugins/bzPopper_7c5ff52.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/home/js/nu_instant_search_f7b49e5.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/plugins/swfobject_0178953.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/soutu/js/tu_68114f1.js https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/amd_modules/@baidu/search-sug_5b9188b.js https://sp1.baidu.com/-L-Xsjip0QIZ8tyhnq/v.gif?logactid=1234567890&amp;showTab=10000&amp;opType=showpv&amp;mod=superman%3Alib&amp;submod=index&amp;superver=supernewplus&amp;glogid=2905514690&amp;type=2011&amp;pid=315&amp;isLogin=0&amp;version=PCHome&amp;terminal=PC&amp;qid=2905514810&amp;sid=1460_33224_33058_31254_32971_33098_33101_26350_33198_33265&amp;super_frm=&amp;from_login=&amp;from_reg=&amp;query=&amp;curcard=2&amp;curcardtab=&amp;_r=0.5174681533375083 https://sp2.baidu.com/-L-Ysjip0QIZ8tyhnq/v.gif?mod=superman%3Acomponents&amp;submod=hotsearch&amp;utype=undefined&amp;superver=supernewplus&amp;portrait=undefined&amp;glogid=2905514690&amp;type=2011&amp;pid=315&amp;isLogin=0&amp;version=PCHome&amp;terminal=PC&amp;qid=2905514810&amp;sid=1460_33224_33058_31254_32971_33098_33101_26350_33198_33265&amp;super_frm=&amp;from_login=&amp;from_reg=&amp;query=&amp;curcard=2&amp;curcardtab=&amp;_r=0.3818542391730819&amp;m=superman%3Acomponents_hotsearchShow&amp;showType=hotword&amp;words=%5B%2231%E7%9C%81%E5%8C%BA%E5%B8%82%E6%96%B0%E5%A2%9E24%E4%BE%8B%E7%A1%AE%E8%AF%8A%3A%E6%9C%AC%E5%9C%9F5%E4%BE%8B%22%2C%22%E9%9F%A9%E6%B0%91%E9%97%B4%E4%BA%BA%E5%A3%AB%E8%B5%B7%E8%AF%89%E4%B8%AD%E9%9F%A9%E6%94%BF%E5%BA%9C%E8%B4%A5%E8%AF%89%22%2C%22%E7%B4%A0%E5%AA%9B%E6%A1%88%E7%BD%AA%E7%8A%AF%E5%88%B0%E5%AE%B6%E7%94%BB%E9%9D%A2%3A%E8%AD%A6%E5%AF%9F%E5%A0%B5%E9%97%A8%E4%BF%9D%E6%8A%A4%22%2C%22%E5%AE%98%E6%96%B9%E9%80%9A%E6%8A%A5%E6%95%99%E7%BB%83%E7%BD%9A%E5%AD%A6%E7%94%9F%E7%94%A8%E4%BE%BF%E6%B1%A0%E6%B0%B4%E6%B4%97%E8%84%B8%22%2C%22%E6%8B%9C%E7%99%BB%3A39%E5%A4%A9%E5%90%8E%E7%BE%8E%E5%B0%86%E9%87%8D%E6%96%B0%E5%8A%A0%E5%85%A5%E5%B7%B4%E9%BB%8E%E5%8D%8F%E5%AE%9A%22%2C%22%E9%BB%91%E9%BE%99%E6%B1%9F%E6%96%B0%E5%A2%9E4%E4%BE%8B%E6%9C%AC%E5%9C%9F%E7%97%85%E4%BE%8B%20%E5%90%AB2%E5%B9%BC%E7%AB%A5%22%5D&amp;pagenum=0 https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/font/iconfont-8db5f471f4.woff2 https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/static/protocol/https/soutu/css/soutu_new2_ae491b7.css https://www.baidu.com/sugrec?prod=pc_his&amp;from=pc_web&amp;json=1&amp;sid=1463_33244_33061_32971_33099_33101_32846_33199_33240&amp;hisdata=&amp;_t=1607828626043&amp;req=2&amp;csor=0 https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/searchbox/nicon-10750f3f7d.png https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/components/tips-e2ceadd14d.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/super_load-a97cbd2188.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/components/qrcode-da919182da.js https://dss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/js/components/guide-13c605998f.js 修改前端响应 利用 route 方法，我们可以实现一些网络劫持和修改操作，比如修改 request 的属性，修改 response 响应结果等。 举例： from playwright.sync_api import sync_playwright import re with sync_playwright() as p: browser = p.chromium.launch(headless=False) page = browser.new_page() def cancel_request(route, request): route.abort() page.route(re.compile(r&quot;(\\.png)|(\\.jpg)&quot;), cancel_request) page.goto(&quot;https://spa6.scrape.center/&quot;) page.wait_for_load_state('networkidle') page.screenshot(path='no_picture.png') browser.close() 这里我们调用了 route 方法，第一个参数通过正则表达式传入了匹配的 URL 路径，这里代表的是任何包含 .png 或 .jpg 的链接，遇到这样的请求，会回调 cancel_request 方法处理，cancel_request 方法可以接收两个参数，一个是 route，代表一个 CallableRoute 对象，另外一个是 request，代表 Request 对象。这里我们直接调用了 route 的 abort 方法，取消了这次请求，所以最终导致的结果就是图片的加载全部取消了。 这个设置有什么用呢？其实是有用的，因为图片资源都是二进制文件，而我们在做爬取过程中可能并不想关心其具体的二进制文件的内容，可能只关心图片的 URL 是什么，所以在浏览器中是否把图片加载出来就不重要了。所以如此设置之后，我们可以提高整个页面的加载速度，提高爬取效率。 另外，利用这个功能，我们还可以将一些响应内容进行修改，比如直接修改 Response 的结果为自定义的文本文件内容，或是构造某些特定的响应返回给前端，用于测试前端在特定响应值情况下的表现。 总结 个人体验是 Playwright 相比于 Selenium 或是 Pyppeteer，安装和使用都更加简便，不需要额外的 webdriver 或是特定版本的浏览器。 最大的优点可能是 codegen 功能，能够大大缩短浏览器自动化代码编写的时间，并且其语法也相对简单，另外，记录所有页面请求、网络劫持修改等功能也非常实用。 缺点是离线环境下的部署安装可能比较麻烦，仍待研究。 ","link":"https://hill-jiang.github.io/post/playwright/"},{"title":"添加“在此处打开windowsTerminal”到右键菜单","content":"win7系统中，在某文件夹中按住shift+右键，可以看到“在此处打开cmd”，这个功能在win10中变成了“在此处打开powershell”。 微软在win10推出自家的高颜值命令行工具windows Terminal后，我便主要使用该工具进行命令行的操作，但是由于没有上述的在文件夹中打开的快捷方式，一直苦于路径的切换。终于，参考了Terminal官方github的issues，摸索出了添加“在此处打开windows Terminal”到右键菜单的方法。 Step 1 查看文档路径： echo %USERPROFILE% echo %LOCALAPPDATA% 一般情况下，结果如下（[username]为用户名）： C:\\Users\\[username] C:\\Users\\[username]\\AppData\\Local Step 2 在cmd中运行以下命令，新建一个文件夹用于存放图标： mkdir %LOCALAPPDATA%\\terminal Step 3 下载图标wt_32.ico并放到上述文件夹下 下载路径：https://github.com/yanglr/WindowsDevTools/tree/master/awosometerminal/icons Step 4 复制以下文本，保存为wt.reg，然后以管理员身份运行（注意修改[username]为自己的用户名） Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\wt] @=&quot;Windows terminal here&quot; &quot;Icon&quot;=&quot;C:\\\\sers\\\\[username]\\\\AppData\\\\Local\\\\terminal\\\\wt_32.ico&quot; [HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\wt\\command] @=&quot;C:\\\\Users\\\\[username]\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\wt.exe&quot; Step 5 打开Terminal，修改profile.json，在cmd的配置中添加以下配置： &quot;satrtingDirectory&quot;: &quot;1&quot; ","link":"https://hill-jiang.github.io/post/添加“在此处打开windowsTerminal”到右键菜单/"}]}